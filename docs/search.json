[
  {
    "objectID": "presentation/presentation.html#project-intention",
    "href": "presentation/presentation.html#project-intention",
    "title": "Predicting Housing Sales",
    "section": "Project Intention",
    "text": "Project Intention\nWhy it Matters\nPhiladelphia’s current Automated Valuation Model (AVM) is underperforming, leading to inconsistent and inequitable property assessments.\nOur Guiding Question\nWhat key predictors are missing from the legacy model, and how can we improve its accuracy and fairness?"
  },
  {
    "objectID": "presentation/presentation.html#data-sources",
    "href": "presentation/presentation.html#data-sources",
    "title": "Predicting Housing Sales",
    "section": "Data Sources",
    "text": "Data Sources\n\nCity of Philadelphia\n\nPhiladelphia Properties and Current Assessments\n\nOpenDataPhilly\n\nNeighborhood Boundaries (Polygon)\nSchool Locations, Attendance, and Withdrawals\n\nGeofabrik (OpenStreetMap)\n\nCommercial and Economic Activity Points\n\nU.S. Census (ACS 5-Year)\n\nEducation\nVacancy Rates\nFamily Household Share\nPercent White\nMedian Household Income"
  },
  {
    "objectID": "presentation/presentation.html#mapped-observations",
    "href": "presentation/presentation.html#mapped-observations",
    "title": "Predicting Housing Sales",
    "section": "Mapped observations",
    "text": "Mapped observations\n\n\n\n\n\nHome Price Across the City\n\n\n\nKey Findings\n\nPrices cluster spatially\nCenter City & NW Philly are highest\nSouth Philly shows additional hotspots"
  },
  {
    "objectID": "presentation/presentation.html#price-drivers",
    "href": "presentation/presentation.html#price-drivers",
    "title": "Predicting Housing Sales",
    "section": "Price Drivers",
    "text": "Price Drivers\n\nEconomic Density\nLivable Space\nBedrooms v Bathrooms\nBuilding Age"
  },
  {
    "objectID": "presentation/presentation.html#stepwise-modeling-strategy",
    "href": "presentation/presentation.html#stepwise-modeling-strategy",
    "title": "Predicting Housing Sales",
    "section": "Stepwise Modeling Strategy",
    "text": "Stepwise Modeling Strategy\n\nModel 1: Structural features only {style=“color: red;”}\nModel 2: + Census tract demographics {style=“color: orange;”}\nModel 3: + Spatial features (KDE, proximity) {style=“color: blue;”}\nModel 4: + Interactions and neighborhood effects {style=“color: green;”}\n\nGoal: Test whether place-based factors improve predictive accuracy.\n\\[\n\\begin{aligned}\n\\log(\\text{Price}) = \\beta_0 &+ \\color{red}{\\beta_1 \\log(\\text{Livable Area}) + \\beta_2 \\text{Bedrooms} + \\beta_3 \\text{Bathrooms}} \\\\\n&+ \\color{red}{\\beta_4 \\text{Age} + \\beta_5 \\text{Interior Condition} + \\beta_6 \\text{Quality Grade}} \\\\\n&+ \\color{orange}{\\beta_7 \\text{Median Income} + \\beta_8 \\text{Family HH Ratio} + \\beta_9 \\text{% Bachelors}} \\\\\n&+ \\color{orange}{\\beta_{10} \\text{% Vacant} + \\beta_{11} \\text{% White}} \\\\\n&+ \\color{blue}{\\beta_{12} \\text{EconKDE} + \\beta_{13} \\text{EconKDE}^2 + \\beta_{14} \\text{TreeKDE} + \\beta_{15} \\text{Mean 3NN Dist}} \\\\\n&+ \\color{green}{\\beta_{16} \\text{MHI Quantile} + \\beta_{17} (\\text{MHI Quantile} \\times \\text{Age})} + \\varepsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentation/presentation.html#model-performance-improves-at-each-stage",
    "href": "presentation/presentation.html#model-performance-improves-at-each-stage",
    "title": "Predicting Housing Sales",
    "section": "Model Performance Improves at Each Stage",
    "text": "Model Performance Improves at Each Stage\n\n\n\n\n\n\n\n\nModel\nAdj. R²\nInterpretation\n\n\n\n\n(1) Structural\n0.38\nHouse features alone are not enough\n\n\n(2) + Census\n0.60\nNeighborhood socioeconomic patterns matter\n\n\n(3) + Spatial\n0.61\nLocation and amenities add signal\n\n\n(4) + Interactions\n0.64\nWealthy areas shape how homes retain value\n\n\n\nTakeaway: Spatial + neighborhood effects are essential to explaining price variation in Philadelphia."
  },
  {
    "objectID": "presentation/presentation.html#hardest-neighborhoods-to-predict",
    "href": "presentation/presentation.html#hardest-neighborhoods-to-predict",
    "title": "Predicting Housing Sales",
    "section": "Hardest Neighborhoods to Predict",
    "text": "Hardest Neighborhoods to Predict\n\n\n\n\n\nAffluent Neighborhoods\n\n\n\nKey Insight\n\nAffluent neighborhoods are the least predictable\nHighest incomes cluster in:\n\nCenter City\nNorthwest Philadelphia\nParts of South Philadelphia\n\nPrices influenced by amenities, prestige, and perception, which are hard to quantify"
  },
  {
    "objectID": "presentation/presentation.html#recommendations",
    "href": "presentation/presentation.html#recommendations",
    "title": "Predicting Housing Sales",
    "section": "Recommendations",
    "text": "Recommendations\nThink about spatial variables\nVariables such as economic density and neighborhood play a large role\nRemain Nuanced\nThe influence of a factor will likely vary by neighborhood\nBathrooms &gt; Bedrooms\nImagine a 4 bed 4 BR unit vs a 4 bed 1 BR unit, the different in desirability is exponential"
  },
  {
    "objectID": "presentation/presentation.html#limitations-next-steps",
    "href": "presentation/presentation.html#limitations-next-steps",
    "title": "Predicting Housing Sales",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\nThe inability to quantify word of mouth\nPercieved neighborhood vibe shift can radically alter future housing prices\n\nI.e. Fishtown\n\nThe non-incorporation of Philadelphia’s future plans\nPlanned and ongoing infrastructure development greatly impacts where housing is developed"
  },
  {
    "objectID": "presentation/presentation.html#questions",
    "href": "presentation/presentation.html#questions",
    "title": "Predicting Housing Sales",
    "section": "Questions?",
    "text": "Questions?\n\nThank you!\n\n\nContact Information:\n\nJack Bader\n\njbader14@upenn.edu\n\nMatthew Levy\n\nmblevy@upenn.edu\n\nTim Wen\n\nsw6as@upenn.edu\n\n\n\n\nJoey Cahill\n\ncahill1@upenn.edu\n\nSam Sen\n\nsen1@upenn.edu\n\nYe Zhang\n\nyezhang1@upenn.edu"
  },
  {
    "objectID": "appendix/appendix.html",
    "href": "appendix/appendix.html",
    "title": "Technical Appendix",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nhere() starts at C:/Users/jcahi/OneDrive/Desktop/MUSA 5080/Midterm/MUSA5080-Midterm\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nLoading required package: spatstat.data\n\nLoading required package: spatstat.univar\n\nspatstat.univar 3.1-4\n\nspatstat.geom 3.6-0\n\n\nAttaching package: 'spatstat.geom'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nLoading required package: spatstat.random\n\nspatstat.random 3.4-2\n\nLoading required package: spatstat.explore\n\nLoading required package: nlme\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nspatstat.explore 3.5-3\n\nLoading required package: spatstat.model\n\nLoading required package: rpart\n\nspatstat.model 3.4-2\n\nLoading required package: spatstat.linnet\n\nspatstat.linnet 3.3-2\n\n\nspatstat 3.4-1 \nFor an introduction to spatstat, type 'beginner' \n\n\nterra 1.8.70\n\n\nAttaching package: 'terra'\n\n\nThe following objects are masked from 'package:spatstat.geom':\n\n    area, delaunay, is.empty, rescale, rotate, shift, where.max,\n    where.min\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:tigris':\n\n    blocks\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:knitr':\n\n    spin\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:spatstat.model':\n\n    bc\n\n\nThe following object is masked from 'package:spatstat.geom':\n\n    ellipse\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:terra':\n\n    time&lt;-\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric"
  },
  {
    "objectID": "appendix/appendix.html#load-and-philadelphia-house-sales-data",
    "href": "appendix/appendix.html#load-and-philadelphia-house-sales-data",
    "title": "Technical Appendix",
    "section": "1.1 Load and Philadelphia house sales data",
    "text": "1.1 Load and Philadelphia house sales data\n\n\nCode\n# Load Philly Property Sales data\nphl_sales &lt;- read_csv(here(\"data\", \"raw\", \"opa_properties_public.csv\"))\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 583776 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (38): basements, beginning_point, book_and_page, building_code, buildin...\ndbl  (31): objectid, category_code, census_tract, depth, exempt_building, ex...\nlgl   (7): cross_reference, date_exterior_condition, mailing_address_2, mark...\ndttm  (3): assessment_date, recording_date, sale_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nFilter to residential properties, 2023-2024 sales\n\n\nCode\n# Check data types\n# glimpse(phl_sales)\n\nphl_sales_res_23_24 &lt;- phl_sales |&gt;\n  filter(\n    category_code == 1, # Residential\n    year(sale_date) %in% c(2023, 2024), # 2023-24 sales\n    !is.na(category_code) & !is.na(sale_date) # Handle nulls\n  )\n\n\n\n\nRemove obvious errors\n\n\nCode\nphl_sales_clean &lt;- phl_sales_res_23_24 |&gt;\n  filter(\n    # Some sale_price are unrealistically too low ($0, $1 etc.)\n    sale_price &gt;= 10000,\n    # Exclude homes with 0 bathrooms\n    number_of_bathrooms &gt; 0,\n    # Some areas are unrealistically low (0, 1, etc.)\n    total_area &gt; 1,\n    # Some 0's remain in total_liveable_area after first area filter\n    total_livable_area &gt; 0,\n    # Filter our unrealistic year built\n    year_built &gt;= 1750\n    ) \n\n\n\n\nHandle missing values\n\n\nCode\n# Check how many features have NA values\n# sum(is.na(phl_sales_clean$number_of_bedrooms))\n# sum(is.na(phl_sales_clean$number_of_bathrooms))\n# sum(is.na(phl_sales_clean$total_livable_area))\n# sum(is.na(phl_sales_clean$year_built))\n\n# Remove the 2 observations with NA values for number of bedrooms\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  filter(\n    !is.na(number_of_bedrooms)\n  )\n\n\n\n\nPreliminary Feature Engineering: Age = sale date - year built\n\n\nCode\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  mutate(\n    sale_year = year(sale_date),\n    age = sale_year - year_built\n  )\n\n\n\n\nDocument all cleaning decisions\n\nOur methodology for cleaning the Philadelphia home sales data is to focus on the features used in our model. As a group, we decided on the following independent variables to consider in our data exploration and model building to be: number of bathrooms, number of bedrooms, total livable area, and year built. We recognize that there is some risk of collinearity between these structural features, which will later be monitored and addressed if needed in the model building stage. Additionally, we also had to clean the sales price column since this is the variable we aim to predict in our model.\nFilter for only residential properties & sales made in 2023-24 (per instructions).\nFilter for realistic sales price &gt;= $10,000.\nFilter for houses with at least 1 bathroom. We will keep observations where number of bedrooms = 0 as this likely signifies a studio apartment. However, it is not feasible for homes to have zero bathrooms, so we will enforce a constraint that a home must have at least 1 bathroom to preserve data integrity.\nFilter for realistic total area &gt; 1 sq ft & realistic total livable area &gt; 0 sq ft.\nFilter for year built &gt;= 1750 (some homes were built in year 0).\nHandle missing values: We removed any missing values in our dependent variable of sales price, since it is crucial we have a true and accurate measure for prediction. We also checked which of our predictor variables had NA values after filtering. Only number of bedrooms had 2 remaining NA values. The rest had no NA values. To remedy this, we will remove the 2 observations from our data. Note, if there was substantial missing values in our predictors, we could use strategies such as imputing the NA values with the mean or median to use when building our model.\nPreliminary feature engineering: Rather than using year built in our Automated Valuation Model, it makes more sense to create a new variable age that is equal to the sale date minus the year built. The age variable is often easier to interpret in exploratory plots with the newer houses appearing on the left and older ones on the right. This is primarily a stylistic preference: the overall pattern of the data will remain the same but mirrored."
  },
  {
    "objectID": "appendix/appendix.html#load-secondary-data",
    "href": "appendix/appendix.html#load-secondary-data",
    "title": "Technical Appendix",
    "section": "1.2 Load Secondary Data",
    "text": "1.2 Load Secondary Data\n\nCensus\nPurpose: Pull demographic and housing data at the census tract level for Philadelphia from the 2023 5-year ACS. This data will provide predictors for neighborhood characteristics in our modeling.\nVariables Collected:\nMedian household income (B19013)\nPercentage of family households (B11001)\nEducation attainment: percent of population 25+ with a bachelor’s degree or higher (B15003)\nHousing vacancy rate (B25002)\nRacial composition: percent white (B02001)\n\nLoad Philly Census Data from Previously Retrieved Files\n\n\nCode\n# Relative to project root\ncensus_path &lt;- here(\"data\", \"Philly Census\")\n\ncensus_csv_path &lt;- file.path(census_path, \"philly_tract_metrics.csv\")\ncensus_shp_path &lt;- file.path(census_path, \"philly_tract.shp\")\n\n# Csv with census tract Geo IDs and metrics\nphilly_censustract &lt;- read_csv(census_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_tract_sf &lt;- st_read(census_shp_path, quiet = TRUE)\n\n\n\n\nObserve Summary Statistics from target metrics.x\n\n\nCode\nnumeric_vars &lt;- c(\"median_income\", \"pct_white\", \"pct_bachelors\", \"pct_vacant\")\nphilly_censustract %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  summary()\n\n\n median_income      pct_white      pct_bachelors      pct_vacant     \n Min.   : 13721   Min.   : 0.000   Min.   : 1.504   Min.   :  0.000  \n 1st Qu.: 42469   1st Qu.: 8.826   1st Qu.:16.094   1st Qu.:  5.366  \n Median : 60817   Median :34.740   Median :28.426   Median :  8.516  \n Mean   : 66877   Mean   :37.944   Mean   :36.325   Mean   :  9.844  \n 3rd Qu.: 85298   3rd Qu.:64.202   3rd Qu.:55.345   3rd Qu.: 12.801  \n Max.   :192727   Max.   :95.513   Max.   :96.632   Max.   :100.000  \n NA's   :27       NA's   :17       NA's   :17       NA's   :19       \n\n\nA quick check of the census variables reveals some missing values and lower than epected values in median income. We will note this information but retain the missing values for now to maintain the full pitcure of census blocks.\n\n\n\nCleaning Methodology (Census)\nMedian income: Selected only the estimate column and renamed it for clarity.\nHousehold composition: Pivoted ACS table to wide format, then calculated total households and family households.\nEducation: Pivoted to wide format, summed relevant categories to compute percent of population with a bachelor’s degree or higher.\nVacancy: Pivoted to wide format, calculated percent of homes vacant (vacant_units / total_units * 100).\nRacial composition: Pivoted to wide format, computed percent white.\nMerging: Combined all datasets by GEOID to create a single dataframe philly_blockgroup with all variables.\nGeometry: Pulled census tract shapefiles with ACS geometry and merged with philly_blockgroup to create philly_bg_map.\n\n\nNeighborhood (Polygon)\nReading in Philadelphia Neighborhoods as a shp object. This will allow us to aggregate data on neighborhoods to identify catagorical metrics.\n\n\nCode\nneighborhood_folder &lt;- here(\"data\", \"philadelphia-neighborhoods\")\nneighborhood_path   &lt;- file.path(neighborhood_folder, \"philadelphia-neighborhoods.shp\")\n\n# Read the shapefile\nphilly_neighborhoods &lt;- st_read(neighborhood_path, quiet = TRUE)\n\nhead(philly_neighborhoods)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.23049 ymin: 39.98491 xmax: -75.0156 ymax: 40.11269\nGeodetic CRS:  WGS 84\n             NAME         LISTNAME         MAPNAME Shape_Leng Shape_Area\n1      BRIDESBURG       Bridesburg      Bridesburg   27814.55   44586264\n2       BUSTLETON        Bustleton       Bustleton   48868.46  114050424\n3      CEDARBROOK       Cedarbrook      Cedarbrook   20021.42   24871745\n4   CHESTNUT_HILL    Chestnut Hill   Chestnut Hill   56394.30   79664975\n5      EAST_FALLS       East Falls      East Falls   27400.78   40576888\n6 MOUNT_AIRY_EAST Mount Airy, East East Mount Airy   28845.55   43152470\n                        geometry\n1 POLYGON ((-75.06773 40.0054...\n2 POLYGON ((-75.0156 40.09487...\n3 POLYGON ((-75.18848 40.0727...\n4 POLYGON ((-75.21221 40.0860...\n5 POLYGON ((-75.18476 40.0282...\n6 POLYGON ((-75.18087 40.0432...\n\n\n\n\nCommercial and office points of interests (Amenities)(alternative in the next section if you do not want to download pbf data)\n\n\nCode\n# downloading osm data from geofabrik:https://download.geofabrik.de/north-america/us-northeast.html\n\n#input_pbf &lt;- \"the pdf file downloaded from the link above\"\n\n\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\n# read the full OSM PBF (you can select layer types like points, lines, polygons)\npoi &lt;- oe_read(input_pbf, \n                       boundary = philly_boundary, \n                       boundary_type = \"clipsrc\", \n                       layer = \"points\")  # or \"lines\" / \"multipolygons\"\n\n\nkeywords &lt;- c(\"shop\",\"amenity\",\"office\",\"historic\",\"tourism\",\"healthcare\",\n              \"building\",\"leisure\")\npattern &lt;- paste0(keywords, collapse = \"|\")\n\n# ==== Filter by 'other_tags' ====\nif (\"other_tags\" %in% names(poi)) {\n  poi$other_tags &lt;- iconv(as.character(poi$other_tags), from = \"\", to = \"UTF-8\", sub = \"\")\n  poi$other_tags[is.na(poi$other_tags)] &lt;- \"\"\n  \n  poi_filtered &lt;- poi %&gt;%\n    filter(grepl(pattern, other_tags, ignore.case = TRUE))\n  \n  cat(\"filtered POIs found:\", nrow(poi_filtered), \"of\", nrow(poi), \"\\n\")\n  \n} \n\n\n\n\nAlternative: filtered POI if you donot want to download osm data\n\n\nCode\npoi_path &lt;- here(\"data\", \"filtered poi\")\npoi_shp_path=file.path(poi_path, \"philadelphia_poi_filtered.shp\")\npoi=st_read(poi_shp_path)\n\n\nReading layer `philadelphia_poi_filtered' from data source \n  `C:\\Users\\jcahi\\OneDrive\\Desktop\\MUSA 5080\\Midterm\\MUSA5080-Midterm\\data\\filtered poi\\philadelphia_poi_filtered.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 11161 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27472 ymin: 39.87383 xmax: -74.95777 ymax: 40.13445\nGeodetic CRS:  WGS 84\n\n\n\n\nKernel Density Rasters (Economic activities density)\nInstead of using distance to CBD, we extracted commercial and office points of interests from OpenStreetMap (OSM), and we opreate a Kernel Density Estimation (KDE) with a bandwidth of 300 meters. By doing that, we manage to get a surface of density of economic activities across the whole city. The higher the KDE value is, the more economic activities it will be, implying a higher likelyhood of the area as a city centers.\nThere are several benefits using this approach compared to distance to CBD. First, with the development of suburbanization, even within the context of Philadelphia County, there is still a shift from monocentric model to polycentric model, meaning there multiple centers/subcenters. Using one CBD fail to capture these subcenters, which may also influence housing price. Second, CBD is an area rather than a point, distance method fail to capture this while the continuous surface computed by KDE would have a value of economic activities across the whole city.\n\n\nCode\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\nphilly_boundary &lt;- st_transform(philly_boundary, 2272)  \npoi &lt;- st_transform(poi, 2272)\n# ==== Prepare point pattern ====\n# Convert sf points to spatstat ppp object\nwin &lt;- as.owin(st_union(philly_boundary))  # window from county boundary\ncoords &lt;- st_coordinates(poi)\npp &lt;- ppp(x = coords[,1], y = coords[,2], window = win)\n\n\nWarning: data contain duplicated points\n\n\nCode\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map &lt;- density.ppp(pp, sigma = 300* 3.28084, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_Economic &lt;- rast(density_map)\ncrs(r_Economic) &lt;- st_crs(philly_boundary)$proj4string\nr_Economic &lt;- mask(r_Economic, vect(philly_boundary))\n\n\n\n\nEducation\nWe used two datasets from OpenDataPhilly.com to identify schools geolocation and populated the metrics off Attendance percent and Withdrawal volumes from those schools.\n\n\nCode\n# Relative to project root\neducation_path &lt;- here(\"data\", \"Education\")\n\neducation_csv_path &lt;- file.path(education_path, \"philadelphia_schools.csv\")\neducation_shp_path &lt;- file.path(education_path, \"Schools Shape\", \"Schools.shp\")\n\n# Csv with School Names and metrics\nphilly_schools &lt;- read_csv(education_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_schools_sf &lt;- st_read(education_shp_path, quiet = TRUE)\n\n\nWe joined the csv file containing the metrics with the shp file containing geoloaction.\n\n\nCode\n# Joining Schools csv metrics to shp file. Joined on 'location_i' (shp) and 'School_code (csv)\n# Keeping metrics for Attendance and Withdrawals\n\n# Select relevant metrics from CSV\nschool_metrics &lt;- philly_schools %&gt;%\n  select(School_code, Attendance, Withdrawals) %&gt;%\n  mutate(School_code = as.character(School_code))\n\nphilly_schools_sf &lt;- philly_schools_sf %&gt;%\n  left_join(school_metrics,\n            by = c(\"location_i\" = \"School_code\"))\n\n\n\n\nCode\nphilly_schools_sf_clean &lt;- philly_schools_sf %&gt;%\n  filter(!is.na(Attendance) & !is.na(Withdrawals))\n\n\n\n\nCode\nnames(philly_schools_sf_clean)\n\n\n [1] \"aun\"         \"school_num\"  \"location_i\"  \"school_nam\"  \"school_n_1\" \n [6] \"street_add\"  \"zip_code\"    \"phone_numb\"  \"grade_leve\"  \"grade_org\"  \n[11] \"enrollment\"  \"type\"        \"type_speci\"  \"objectid\"    \"Attendance\" \n[16] \"Withdrawals\" \"geometry\"   \n\n\nCode\nnrow(philly_schools_sf_clean)\n\n\n[1] 204\n\n\nOnce joined, we dropped rows that did not have values in Attendance and Withdrawal. This resulted in 204 public schools and their metrics located in Philadelphia City Limits.\n###Tree density Location of trees data was extracted from Opendata Philly. A Kernel Density Estimation was used to estimate the density of trees. The higher the value is, the more trees there will be in this (and surronding) cell\n\n\nCode\ntree_path &lt;- here(\"data\", \"ppr_tree_inventory_2024\")\n\ntree_shp_path &lt;- file.path(tree_path, \"ppr_tree_inventory_2024.shp\")\n\n\ntrees=st_read(tree_shp_path)\n\n\nReading layer `ppr_tree_inventory_2024' from data source \n  `C:\\Users\\jcahi\\OneDrive\\Desktop\\MUSA 5080\\Midterm\\MUSA5080-Midterm\\data\\ppr_tree_inventory_2024\\ppr_tree_inventory_2024.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 151713 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8380434 ymin: 4847791 xmax: -8344373 ymax: 4885938\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nCode\ntrees=st_transform(trees,2272)\n\n# Convert sf points to spatstat ppp object\ncoords_trees &lt;- st_coordinates(trees)\npp_trees &lt;- ppp(x = coords_trees[,1], y = coords_trees[,2], window = win)\n\n\nWarning: 355 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n\nCode\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map_trees &lt;- density.ppp(pp_trees, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_trees &lt;- rast(density_map_trees)\ncrs(r_trees) &lt;- st_crs(philly_boundary)$proj4string\nr_trees &lt;- mask(r_trees, vect(philly_boundary))  # mask to county boundary\n\n\n\n\nJoining data together\nFirst, since commercial and office POI and trees are point data, and the more they cluster the higher the housing price will be. As a result, we use Kernel Density method to estimate the density of them. Value of the cell was assign to the point (housing prices), if the point falls within it.\nSecond, as census tracts and neighborhood are polygon data, a st_within spatial join was used to join them with housing data. Values from polygon data will assign to the points when points fall within it.\n\n\nCode\n# Geometry - Cenus Data Merge\nphilly_tract_sf$GEOID=as.numeric(philly_tract_sf$GEOID)\nphilly_tract_map &lt;- philly_tract_sf %&gt;%\n  left_join(philly_censustract, by = \"GEOID\")\n\n#convert housing prices data into point data\nphl_sales_clean_sf = phl_sales_clean%&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%   # parse WKT into geometry\n  st_as_sf(crs = 2272)  \n#convert and match the crs\nphilly_schools_sf_clean=philly_schools_sf_clean%&gt;%\n  st_transform(2272)\nphilly_neighborhoods=philly_neighborhoods%&gt;%\n  st_transform(2272)\nphilly_tract_map=philly_tract_map%&gt;%\n  st_transform(2272)\n#merge them together\nphl_sales_clean_sf_final=phl_sales_clean_sf%&gt;%\n  st_join(philly_tract_map,join=st_within)%&gt;%\n  st_join(philly_neighborhoods,join = st_within)\nphl_sales_clean_sf_final &lt;- st_transform(phl_sales_clean_sf_final, crs = st_crs(r_trees))\nphl_sales_clean_sf_final$EconKDE &lt;- raster::extract(r_Economic,phl_sales_clean_sf_final)\nphl_sales_clean_sf_final$TreeKDE &lt;- raster::extract(r_trees, phl_sales_clean_sf_final)  \nphl_sales_clean_sf_final=phl_sales_clean_sf_final%&gt;%\n  st_transform(2272)\n\n\n\n\nSummary table before and after dimensions\n\n\nCode\nbefore_after_summary &lt;- data.frame(\n  Stage = c(\"Raw Data\", \n            \"After Residential Filter (2023–24)\", \n            \"After Removing Errors & NAs\", \n            \"After Spatial Joins & Final Cleaning\"),\n  Rows = c(nrow(phl_sales),\n           nrow(phl_sales_res_23_24),\n           nrow(phl_sales_clean),\n           nrow(phl_sales_clean_sf_final)),\n  Columns = c(ncol(phl_sales),\n              ncol(phl_sales_res_23_24),\n              ncol(phl_sales_clean),\n              ncol(phl_sales_clean_sf_final))\n)\n\nknitr::kable(before_after_summary, caption = \"Data dimensions before and after cleaning\")\n\n\n\nData dimensions before and after cleaning\n\n\nStage\nRows\nColumns\n\n\n\n\nRaw Data\n583776\n79\n\n\nAfter Residential Filter (2023–24)\n34567\n79\n\n\nAfter Removing Errors & NAs\n22034\n81\n\n\nAfter Spatial Joins & Final Cleaning\n22034\n106"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Project Overview:\n\nMembers: Jack Bader, Date: October 27, 2025"
  }
]