[
  {
    "objectID": "presentation/presentation.html#key-issues",
    "href": "presentation/presentation.html#key-issues",
    "title": "Predicting Housing Sales",
    "section": "Key Issues",
    "text": "Key Issues\n\nThe City of Philadelphia’s current automated valuation model (AVM) is underpreforming\n\n\nSo what factors strongly influence housing price that the city could be overlooking?"
  },
  {
    "objectID": "presentation/presentation.html#data-sources",
    "href": "presentation/presentation.html#data-sources",
    "title": "Predicting Housing Sales",
    "section": "Data Sources",
    "text": "Data Sources\n\nCity of Philadelphia\nPhiladelphia Properties and Current Assessments\nOpenDataPhilly\n\nPhiladelphia Neighborhoods Polygon\nSchool attendance, withdrawls, location\n\nGeofrabrik\n\nEconomic activity locations\n\nUS Census:\n\n5-year ACS data\n\nEducation, vacancy, family share, percent white, median income"
  },
  {
    "objectID": "presentation/presentation.html#a-mapped-observations",
    "href": "presentation/presentation.html#a-mapped-observations",
    "title": "Predicting Housing Sales",
    "section": "A mapped observations",
    "text": "A mapped observations\n\nMedian houseing prices are clustered in specific areas of the City\nCenter City, Northwest Philadelphia, and select areas of South Philadelphia see the highest home prices"
  },
  {
    "objectID": "presentation/presentation.html#price-drivers",
    "href": "presentation/presentation.html#price-drivers",
    "title": "Predicting Housing Sales",
    "section": "Price Drivers",
    "text": "Price Drivers\n\nEconomic Density\nLivable Space\nBedrooms v Bathrooms\nBuilding Age"
  },
  {
    "objectID": "presentation/presentation.html#model-comparison",
    "href": "presentation/presentation.html#model-comparison",
    "title": "Predicting Housing Sales",
    "section": "Model Comparison",
    "text": "Model Comparison"
  },
  {
    "objectID": "presentation/presentation.html#hardest-neighborhoods-to-predict",
    "href": "presentation/presentation.html#hardest-neighborhoods-to-predict",
    "title": "Predicting Housing Sales",
    "section": "Hardest Neighborhoods to Predict",
    "text": "Hardest Neighborhoods to Predict\nAffluent neigborhoods\nNeighborhood affluence concentrated in areas around Center City, Northwest Philadelphia, and South Philadelphia"
  },
  {
    "objectID": "presentation/presentation.html#recommendations",
    "href": "presentation/presentation.html#recommendations",
    "title": "Predicting Housing Sales",
    "section": "Recommendations",
    "text": "Recommendations\nThink about spatial variables\nVariables such as economic density and neighborhood play a large role\nRemain Nuanced\nThe influence of a factor will likely vary by neighborhood\nBathrooms &gt; Bedrooms\nImagine a 4 bed 4 BR unit vs a 4 bed 1 BR unit, the different in desirability is exponential"
  },
  {
    "objectID": "presentation/presentation.html#limitations-next-steps",
    "href": "presentation/presentation.html#limitations-next-steps",
    "title": "Predicting Housing Sales",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\nThe inability to quantify word of mouth\nPercieved neighborhood vibe shift can radically alter future housing prices\n\nI.e. Fishtown\n\nThe non-incorporation of Philadelphia’s future plans\nPlanned and ongoing infrastructure development greatly impacts where housing is developed"
  },
  {
    "objectID": "presentation/presentation.html#questions",
    "href": "presentation/presentation.html#questions",
    "title": "Predicting Housing Sales",
    "section": "Questions?",
    "text": "Questions?\n\nThank you!\n\n\nContact Information:\n\nJack Bader\n\njbader14@upenn.edu\n\nMatthew Levy\n\nmblevy@upenn.edu\n\nTim Wen\n\nsw6as@upenn.edu\n\n\n\n\nJoey Cahill\n\ncahill1@upenn.edu\n\nSam Sen\n\nsen1@upenn.edu\n\nYe Zhang\n\nyezhang1@upenn.edu"
  },
  {
    "objectID": "appendix/appendix.html",
    "href": "appendix/appendix.html",
    "title": "Technical Appendix",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n载入程序包：'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nhere() starts at C:/Users/wensh/Desktop/MUSA5080-Midterm\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\n载入需要的程序包：spatstat.data\n\n载入需要的程序包：spatstat.univar\n\nspatstat.univar 3.1-4\n\nspatstat.geom 3.6-0\n\n\n载入程序包：'spatstat.geom'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\n载入需要的程序包：spatstat.random\n\nspatstat.random 3.4-2\n\n载入需要的程序包：spatstat.explore\n\n载入需要的程序包：nlme\n\n\n载入程序包：'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nspatstat.explore 3.5-3\n\n载入需要的程序包：spatstat.model\n\n载入需要的程序包：rpart\n\nspatstat.model 3.4-2\n\n载入需要的程序包：spatstat.linnet\n\nspatstat.linnet 3.3-2\n\n\nspatstat 3.4-1 \nFor an introduction to spatstat, type 'beginner' \n\n\nterra 1.8.60\n\n\n载入程序包：'terra'\n\n\nThe following objects are masked from 'package:spatstat.geom':\n\n    area, delaunay, is.empty, rescale, rotate, shift, where.max,\n    where.min\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:tigris':\n\n    blocks\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:knitr':\n\n    spin\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\n载入程序包：'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\n载入需要的程序包：carData\n\n\n载入程序包：'car'\n\n\nThe following object is masked from 'package:spatstat.model':\n\n    bc\n\n\nThe following object is masked from 'package:spatstat.geom':\n\n    ellipse\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\n载入需要的程序包：zoo\n\n\n载入程序包：'zoo'\n\n\nThe following object is masked from 'package:terra':\n\n    time&lt;-\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric"
  },
  {
    "objectID": "appendix/appendix.html#load-and-philadelphia-house-sales-data",
    "href": "appendix/appendix.html#load-and-philadelphia-house-sales-data",
    "title": "Technical Appendix",
    "section": "1.1 Load and Philadelphia house sales data",
    "text": "1.1 Load and Philadelphia house sales data\n\n# Load Philly Property Sales data\nphl_sales &lt;- read_csv(here(\"data\", \"raw\", \"opa_properties_public.csv\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 583776 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (38): basements, beginning_point, book_and_page, building_code, buildin...\ndbl  (31): objectid, category_code, census_tract, depth, exempt_building, ex...\nlgl   (7): cross_reference, date_exterior_condition, mailing_address_2, mark...\ndttm  (3): assessment_date, recording_date, sale_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nFilter to residential properties, 2023-2024 sales\n\n# Check data types\n# glimpse(phl_sales)\n\nphl_sales_res_23_24 &lt;- phl_sales |&gt;\n  filter(\n    category_code == 1, # Residential\n    year(sale_date) %in% c(2023, 2024), # 2023-24 sales\n    !is.na(category_code) & !is.na(sale_date) # Handle nulls\n  )\n\n\n\nRemove obvious errors\n\nphl_sales_clean &lt;- phl_sales_res_23_24 |&gt;\n  filter(\n    # Some sale_price are unrealistically too low ($0, $1 etc.)\n    sale_price &gt;= 10000,\n    # Exclude homes with 0 bathrooms\n    number_of_bathrooms &gt; 0,\n    # Some areas are unrealistically low (0, 1, etc.)\n    total_area &gt; 1,\n    # Some 0's remain in total_liveable_area after first area filter\n    total_livable_area &gt; 0,\n    # Filter our unrealistic year built\n    year_built &gt;= 1750\n    ) \n\n\n\nHandle missing values\n\n# Check how many features have NA values\n# sum(is.na(phl_sales_clean$number_of_bedrooms))\n# sum(is.na(phl_sales_clean$number_of_bathrooms))\n# sum(is.na(phl_sales_clean$total_livable_area))\n# sum(is.na(phl_sales_clean$year_built))\n\n# Remove the 2 observations with NA values for number of bedrooms\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  filter(\n    !is.na(number_of_bedrooms)\n  )\n\n\n\nPreliminary Feature Engineering: Age = sale date - year built\n\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  mutate(\n    sale_year = year(sale_date),\n    age = sale_year - year_built\n  )\n\n\n\nDocument all cleaning decisions\n\nOur methodology for cleaning the Philadelphia home sales data is to focus on the features used in our model. As a group, we decided on the following independent variables to consider in our data exploration and model building to be: number of bathrooms, number of bedrooms, total livable area, and year built. We recognize that there is some risk of collinearity between these structural features, which will later be monitored and addressed if needed in the model building stage. Additionally, we also had to clean the sales price column since this is the variable we aim to predict in our model.\nFilter for only residential properties & sales made in 2023-24 (per instructions).\nFilter for realistic sales price &gt;= $10,000.\nFilter for houses with at least 1 bathroom. We will keep observations where number of bedrooms = 0 as this likely signifies a studio apartment. However, it is not feasible for homes to have zero bathrooms, so we will enforce a constraint that a home must have at least 1 bathroom to preserve data integrity.\nFilter for realistic total area &gt; 1 sq ft & realistic total livable area &gt; 0 sq ft.\nFilter for year built &gt;= 1750 (some homes were built in year 0).\nHandle missing values: We removed any missing values in our dependent variable of sales price, since it is crucial we have a true and accurate measure for prediction. We also checked which of our predictor variables had NA values after filtering. Only number of bedrooms had 2 remaining NA values. The rest had no NA values. To remedy this, we will remove the 2 observations from our data. Note, if there was substantial missing values in our predictors, we could use strategies such as imputing the NA values with the mean or median to use when building our model.\nPreliminary feature engineering: Rather than using year built in our Automated Valuation Model, it makes more sense to create a new variable age that is equal to the sale date minus the year built. The age variable is often easier to interpret in exploratory plots with the newer houses appearing on the left and older ones on the right. This is primarily a stylistic preference: the overall pattern of the data will remain the same but mirrored."
  },
  {
    "objectID": "appendix/appendix.html#load-secondary-data",
    "href": "appendix/appendix.html#load-secondary-data",
    "title": "Technical Appendix",
    "section": "1.2 Load Secondary Data",
    "text": "1.2 Load Secondary Data\n\nCensus\nPurpose: Pull demographic and housing data at the census tract level for Philadelphia from the 2023 5-year ACS. This data will provide predictors for neighborhood characteristics in our modeling.\nVariables Collected:\nMedian household income (B19013)\nPercentage of family households (B11001)\nEducation attainment: percent of population 25+ with a bachelor’s degree or higher (B15003)\nHousing vacancy rate (B25002)\nRacial composition: percent white (B02001)\n\nLoad Philly Census Data from Previously Retrieved Files\n\n# Relative to project root\ncensus_path &lt;- here(\"data\", \"Philly Census\")\n\ncensus_csv_path &lt;- file.path(census_path, \"philly_tract_metrics.csv\")\ncensus_shp_path &lt;- file.path(census_path, \"philly_tract.shp\")\n\n# Csv with census tract Geo IDs and metrics\nphilly_censustract &lt;- read_csv(census_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_tract_sf &lt;- st_read(census_shp_path, quiet = TRUE)\n\n\n\nObserve Summary Statistics from target metrics.x\n\nnumeric_vars &lt;- c(\"median_income\", \"pct_white\", \"pct_bachelors\", \"pct_vacant\")\nphilly_censustract %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  summary()\n\n median_income      pct_white      pct_bachelors      pct_vacant     \n Min.   : 13721   Min.   : 0.000   Min.   : 1.504   Min.   :  0.000  \n 1st Qu.: 42469   1st Qu.: 8.826   1st Qu.:16.094   1st Qu.:  5.366  \n Median : 60817   Median :34.740   Median :28.426   Median :  8.516  \n Mean   : 66877   Mean   :37.944   Mean   :36.325   Mean   :  9.844  \n 3rd Qu.: 85298   3rd Qu.:64.202   3rd Qu.:55.345   3rd Qu.: 12.801  \n Max.   :192727   Max.   :95.513   Max.   :96.632   Max.   :100.000  \n NA's   :27       NA's   :17       NA's   :17       NA's   :19       \n\n\nA quick check of the census variables reveals some missing values and lower than epected values in median income. We will note this information but retain the missing values for now to maintain the full pitcure of census blocks.\n\n\n\nCleaning Methodology (Census)\nMedian income: Selected only the estimate column and renamed it for clarity.\nHousehold composition: Pivoted ACS table to wide format, then calculated total households and family households.\nEducation: Pivoted to wide format, summed relevant categories to compute percent of population with a bachelor’s degree or higher.\nVacancy: Pivoted to wide format, calculated percent of homes vacant (vacant_units / total_units * 100).\nRacial composition: Pivoted to wide format, computed percent white.\nMerging: Combined all datasets by GEOID to create a single dataframe philly_blockgroup with all variables.\nGeometry: Pulled census tract shapefiles with ACS geometry and merged with philly_blockgroup to create philly_bg_map.\n\n\nNeighborhood (Polygon)\nReading in Philadelphia Neighborhoods as a shp object. This will allow us to aggregate data on neighborhoods to identify catagorical metrics.\n\nneighborhood_folder &lt;- here(\"data\", \"philadelphia-neighborhoods\")\nneighborhood_path   &lt;- file.path(neighborhood_folder, \"philadelphia-neighborhoods.shp\")\n\n# Read the shapefile\nphilly_neighborhoods &lt;- st_read(neighborhood_path, quiet = TRUE)\n\nhead(philly_neighborhoods)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.23049 ymin: 39.98491 xmax: -75.0156 ymax: 40.11269\nGeodetic CRS:  WGS 84\n             NAME         LISTNAME         MAPNAME Shape_Leng Shape_Area\n1      BRIDESBURG       Bridesburg      Bridesburg   27814.55   44586264\n2       BUSTLETON        Bustleton       Bustleton   48868.46  114050424\n3      CEDARBROOK       Cedarbrook      Cedarbrook   20021.42   24871745\n4   CHESTNUT_HILL    Chestnut Hill   Chestnut Hill   56394.30   79664975\n5      EAST_FALLS       East Falls      East Falls   27400.78   40576888\n6 MOUNT_AIRY_EAST Mount Airy, East East Mount Airy   28845.55   43152470\n                        geometry\n1 POLYGON ((-75.06773 40.0054...\n2 POLYGON ((-75.0156 40.09487...\n3 POLYGON ((-75.18848 40.0727...\n4 POLYGON ((-75.21221 40.0860...\n5 POLYGON ((-75.18476 40.0282...\n6 POLYGON ((-75.18087 40.0432...\n\n\n\n\nCommercial and office points of interests (Amenities)(alternative at line 360 if you do not want to download pbf data)\n\n# downloading osm data from geofabrik:https://download.geofabrik.de/north-america/us-northeast.html\n\n#input_pbf &lt;- \"the pdf file downloaded from the link above\"\n\n\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\n# read the full OSM PBF (you can select layer types like points, lines, polygons)\npoi &lt;- oe_read(input_pbf, \n                       boundary = philly_boundary, \n                       boundary_type = \"clipsrc\", \n                       layer = \"points\")  # or \"lines\" / \"multipolygons\"\n\n\nkeywords &lt;- c(\"shop\",\"amenity\",\"office\",\"historic\",\"tourism\",\"healthcare\",\n              \"building\",\"leisure\")\npattern &lt;- paste0(keywords, collapse = \"|\")\n\n# ==== Filter by 'other_tags' ====\nif (\"other_tags\" %in% names(poi)) {\n  poi$other_tags &lt;- iconv(as.character(poi$other_tags), from = \"\", to = \"UTF-8\", sub = \"\")\n  poi$other_tags[is.na(poi$other_tags)] &lt;- \"\"\n  \n  poi_filtered &lt;- poi %&gt;%\n    filter(grepl(pattern, other_tags, ignore.case = TRUE))\n  \n  cat(\"filtered POIs found:\", nrow(poi_filtered), \"of\", nrow(poi), \"\\n\")\n  \n} \n\n###Alternative: filtered POI if you donot want to download osm data\n\npoi_path &lt;- here(\"data\", \"filtered poi\")\npoi_shp_path=file.path(poi_path, \"philadelphia_poi_filtered.shp\")\npoi=st_read(poi_shp_path)\n\nReading layer `philadelphia_poi_filtered' from data source \n  `C:\\Users\\wensh\\Desktop\\MUSA5080-Midterm\\data\\filtered poi\\philadelphia_poi_filtered.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 11161 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27472 ymin: 39.87383 xmax: -74.95777 ymax: 40.13445\nGeodetic CRS:  WGS 84\n\n\n\n\nKernel Density Rasters (Economic activities density)\nInstead of using distance to CBD, we extracted commercial and office points of interests from OpenStreetMap (OSM), and we opreate a Kernel Density Estimation (KDE) with a bandwidth of 300 meters. By doing that, we manage to get a surface of density of economic activities across the whole city. The higher the KDE value is, the more economic activities it will be, implying a higher likelyhood of the area as a city centers.\nThere are several benefits using this approach compared to distance to CBD. First, with the development of suburbanization, even within the context of Philadelphia County, there is still a shift from monocentric model to polycentric model, meaning there multiple centers/subcenters. Using one CBD fail to capture these subcenters, which may also influence housing price. Second, CBD is an area rather than a point, distance method fail to capture this while the continuous surface computed by KDE would have a value of economic activities across the whole city.\n\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\nphilly_boundary &lt;- st_transform(philly_boundary, 2272)  \npoi &lt;- st_transform(poi, 2272)\n# ==== Prepare point pattern ====\n# Convert sf points to spatstat ppp object\nwin &lt;- as.owin(st_union(philly_boundary))  # window from county boundary\ncoords &lt;- st_coordinates(poi)\npp &lt;- ppp(x = coords[,1], y = coords[,2], window = win)\n\nWarning: data contain duplicated points\n\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map &lt;- density.ppp(pp, sigma = 300* 3.28084, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_Economic &lt;- rast(density_map)\ncrs(r_Economic) &lt;- st_crs(philly_boundary)$proj4string\nr_Economic &lt;- mask(r_Economic, vect(philly_boundary))\n\n\n\nEducation\nWe used two datasets from OpenDataPhilly.com to identify schools geolocation and populated the metrics off Attendance percent and Withdrawal volumes from those schools.\n\n# Relative to project root\neducation_path &lt;- here(\"data\", \"Education\")\n\neducation_csv_path &lt;- file.path(education_path, \"philadelphia_schools.csv\")\neducation_shp_path &lt;- file.path(education_path, \"Schools Shape\", \"Schools.shp\")\n\n# Csv with School Names and metrics\nphilly_schools &lt;- read_csv(education_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_schools_sf &lt;- st_read(education_shp_path, quiet = TRUE)\n\nWe joined the csv file containing the metrics with the shp file containing geoloaction.\n\n# Joining Schools csv metrics to shp file. Joined on 'location_i' (shp) and 'School_code (csv)\n# Keeping metrics for Attendance and Withdrawals\n\n# Select relevant metrics from CSV\nschool_metrics &lt;- philly_schools %&gt;%\n  select(School_code, Attendance, Withdrawals) %&gt;%\n  mutate(School_code = as.character(School_code))\n\nphilly_schools_sf &lt;- philly_schools_sf %&gt;%\n  left_join(school_metrics,\n            by = c(\"location_i\" = \"School_code\"))\n\n\nphilly_schools_sf_clean &lt;- philly_schools_sf %&gt;%\n  filter(!is.na(Attendance) & !is.na(Withdrawals))\n\n\nnames(philly_schools_sf_clean)\n\n [1] \"aun\"         \"school_num\"  \"location_i\"  \"school_nam\"  \"school_n_1\" \n [6] \"street_add\"  \"zip_code\"    \"phone_numb\"  \"grade_leve\"  \"grade_org\"  \n[11] \"enrollment\"  \"type\"        \"type_speci\"  \"objectid\"    \"Attendance\" \n[16] \"Withdrawals\" \"geometry\"   \n\nnrow(philly_schools_sf_clean)\n\n[1] 204\n\n\nOnce joined, we dropped rows that did not have values in Attendance and Withdrawal. This resulted in 204 public schools and their metrics located in Philadelphia City Limits.\n###Tree density Location of trees data was extracted from Opendata Philly. A Kernel Density Estimation was used to estimate the density of trees. The higher the value is, the more trees there will be in this (and surronding) cell\n\ntree_path &lt;- here(\"data\", \"ppr_tree_inventory_2024\")\n\ntree_shp_path &lt;- file.path(tree_path, \"ppr_tree_inventory_2024.shp\")\n\n\ntrees=st_read(tree_shp_path)\n\nReading layer `ppr_tree_inventory_2024' from data source \n  `C:\\Users\\wensh\\Desktop\\MUSA5080-Midterm\\data\\ppr_tree_inventory_2024\\ppr_tree_inventory_2024.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 151713 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8380434 ymin: 4847791 xmax: -8344373 ymax: 4885938\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ntrees=st_transform(trees,2272)\n\n# Convert sf points to spatstat ppp object\ncoords_trees &lt;- st_coordinates(trees)\npp_trees &lt;- ppp(x = coords_trees[,1], y = coords_trees[,2], window = win)\n\nWarning: 355 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map_trees &lt;- density.ppp(pp_trees, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_trees &lt;- rast(density_map_trees)\ncrs(r_trees) &lt;- st_crs(philly_boundary)$proj4string\nr_trees &lt;- mask(r_trees, vect(philly_boundary))  # mask to county boundary\n\n\n\nJoining data together\nFirst, since commercial and office POI and trees are point data, and the more they cluster the higher the housing price will be. As a result, we use Kernel Density method to estimate the density of them. Value of the cell was assign to the point (housing prices), if the point falls within it.\nSecond, as census tracts and neighborhood are polygon data, a st_within spatial join was used to join them with housing data. Values from polygon data will assign to the points when points fall within it.\n\n# Geometry - Cenus Data Merge\nphilly_tract_sf$GEOID=as.numeric(philly_tract_sf$GEOID)\nphilly_tract_map &lt;- philly_tract_sf %&gt;%\n  left_join(philly_censustract, by = \"GEOID\")\n\n#convert housing prices data into point data\nphl_sales_clean_sf = phl_sales_clean%&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%   # parse WKT into geometry\n  st_as_sf(crs = 2272)  \n#convert and match the crs\nphilly_schools_sf_clean=philly_schools_sf_clean%&gt;%\n  st_transform(2272)\nphilly_neighborhoods=philly_neighborhoods%&gt;%\n  st_transform(2272)\nphilly_tract_map=philly_tract_map%&gt;%\n  st_transform(2272)\n#merge them together\nphl_sales_clean_sf_final=phl_sales_clean_sf%&gt;%\n  st_join(philly_tract_map,join=st_within)%&gt;%\n  st_join(philly_neighborhoods,join = st_within)\nphl_sales_clean_sf_final &lt;- st_transform(phl_sales_clean_sf_final, crs = st_crs(r_trees))\nphl_sales_clean_sf_final$EconKDE &lt;- raster::extract(r_Economic,phl_sales_clean_sf_final)\nphl_sales_clean_sf_final$TreeKDE &lt;- raster::extract(r_trees, phl_sales_clean_sf_final)  \nphl_sales_clean_sf_final=phl_sales_clean_sf_final%&gt;%\n  st_transform(2272)\n\n\n\nSummary table before and after dimensions\n\nbefore_after_summary &lt;- data.frame(\n  Stage = c(\"Raw Data\", \n            \"After Residential Filter (2023–24)\", \n            \"After Removing Errors & NAs\", \n            \"After Spatial Joins & Final Cleaning\"),\n  Rows = c(nrow(phl_sales),\n           nrow(phl_sales_res_23_24),\n           nrow(phl_sales_clean),\n           nrow(phl_sales_clean_sf_final)),\n  Columns = c(ncol(phl_sales),\n              ncol(phl_sales_res_23_24),\n              ncol(phl_sales_clean),\n              ncol(phl_sales_clean_sf_final))\n)\n\nknitr::kable(before_after_summary, caption = \"Table A1. Data dimensions before and after cleaning\")\n\n\nTable A1. Data dimensions before and after cleaning\n\n\nStage\nRows\nColumns\n\n\n\n\nRaw Data\n583776\n79\n\n\nAfter Residential Filter (2023–24)\n34567\n79\n\n\nAfter Removing Errors & NAs\n22034\n81\n\n\nAfter Spatial Joins & Final Cleaning\n22034\n106"
  },
  {
    "objectID": "appendix/appendix.html#diagnostic-plots-for-best-model",
    "href": "appendix/appendix.html#diagnostic-plots-for-best-model",
    "title": "Technical Appendix",
    "section": "Diagnostic Plots for Best Model",
    "text": "Diagnostic Plots for Best Model\n\n#generate diagnostic plots\nplot(model4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Residual Plot:\n#The Residuals vs Fitted plot shows mild heteroskedasticity and a small degree of curvature, which is expected in a dataset of this size, and the model performs consistently well across most of the fitted range with only a few isolated outliers.\n\n#QQ-Plot:\n#The Q-Q plot shows slight deviation from normality in the extreme tails, which is common in large real-world housing datasets, while the majority of residuals align closely with the theoretical normal distribution, indicating that model inference remains reliable."
  },
  {
    "objectID": "appendix/appendix.html#cooks-distance-plot",
    "href": "appendix/appendix.html#cooks-distance-plot",
    "title": "Technical Appendix",
    "section": "Cook’s Distance Plot",
    "text": "Cook’s Distance Plot\n\n#identify influential observations\ncooks_d &lt;- cooks.distance(model4)\nplot(cooks_d, type = \"h\", main = \"Cook's Distance\",\n     ylab = \"Cook's Distance\", xlab = \"Observation\")\nabline(h = 4/nrow(for_model_building), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n#The Cook’s Distance plot shows that while a few observations exert relatively higher influence on the regression model, the vast majority of cases have negligible influence, and no single data point appears to unduly distort the model’s estimated coefficients.\n\n#Phase 7: Conclusions & Recommendations\nOur final model’s is able to explain 64% of the variation on logged (is this right way to describe this?) house prices in Philadelphia, with an average error of 48.31%. Our residuals mostly fall along the Q-Q plot’s line and are homoscadestic (as seen in the Scale-Location plot), meaning that the residuals both fall on a normal distribution and the variance of residuals is consistent across all independent variable values. Our K-folds analysis returned an R2 of .65 (higher than the model’s original .64), illustrating our model’s efficacy with non-test data. The feature that matters most is the economic density around the examined housing unit, as it first has a positive relationship with housing price and then has a negative relationship.\nHardest neighborhoods to predict are the most affluent areas — as the social connotations of these spaces are both largely influential in housing prices as well as unquantifiable. Becuase of historical underdevelopment and the history of how that is tied to both race and class, housing prices are intrinsically tied to these vulnerable groups. A model that predicts housing prices is also convincing developers to focus on already well-to-do neighborhoods, whether it be when building housing or commercial centers. There are pros and cons to this, as it may protect neighborhoods from gentrification, but it could also exclude them from capital infusions and access to goods/services. Our model is limited in numerous ways, one of which is the inability to quantify power of word of mouth. For example, once the general vibe around a neighborhood — such as Fishtown — changes from working class (or from another marginalized group) to “up and coming” or “hip”, future housing prices in the area may radically change. Another limitation is the non-incorporation of Philadelphia’s future plans. For example, if the Roosevelt Boulevard extension was to begin construction, then our model would be unable to predict the change in housing price until the stations were built and the local impacts began."
  },
  {
    "objectID": "appendix/appendix.html#coefficient-interpretations",
    "href": "appendix/appendix.html#coefficient-interpretations",
    "title": "Technical Appendix",
    "section": "Coefficient Interpretations",
    "text": "Coefficient Interpretations\nStructural Features: - Log(Total Livable Area) (β = 0.509, p &lt; 0.001): A 1% increase in total livable area is associated with a 0.509% increase in sale price, holding all other factors constant. This represents the strongest predictor in our model, indicating that size is the primary driver of housing value in Philadelphia.\n\nNumber of Bedrooms (β = 0.018, p &lt; 0.001): Each additional bedroom is associated with approximately a 1.8% increase in sale price. This positive coefficient suggests that bedrooms add value, though the effect is modest compared to other structural features.\nNumber of Bathrooms (β = 0.151, p &lt; 0.001): Each additional bathroom is associated with approximately a 16.3% increase in sale price. This substantial effect indicates that bathrooms are highly valued amenities in Philadelphia’s housing market, likely reflecting both functional utility and luxury appeal.\nBuilding Age (β = -0.004, p &lt; 0.001): Each additional year of building age is associated with approximately a 0.4% decrease in sale price. This suggests that newer homes command premium prices, reflecting depreciation over time and the value of modern amenities and construction standards.\nInterior Condition (β = -0.164, p &lt; 0.001): Each unit increase in interior condition rating (where higher numbers indicate worse condition) is associated with approximately a 17.9% decrease in sale price. This substantial effect demonstrates the critical importance of property condition in determining market value.\n\nNeighborhood Demographics: - Median Household Income (β = 0.00000, p &lt; 0.1): While statistically significant, the coefficient is extremely small, suggesting that tract-level median income has a minimal direct effect on individual property prices when controlling for other factors.\n\n% Bachelor’s Degree Holders (β = 0.008, p &lt; 0.001): A 1 percentage point increase in the proportion of residents with bachelor’s degrees is associated with approximately a 0.8% increase in sale price. This reflects the premium associated with highly educated neighborhoods or higher ability to pay for degree-holding buyers.\n% Vacant Housing Units (β = -0.011, p &lt; 0.001): A 1 percentage point increase in vacancy rate is associated with approximately a 1.1% decrease in sale price. This negative relationship reflects the detrimental effects of neighborhood blight and disinvestment on property values.\n% White Population (β = 0.004, p &lt; 0.001): A 1 percentage point increase in the white population share is associated with approximately a 0.4% increase in sale price. This coefficient likely captures both historical patterns of neighborhood investment and ongoing racial disparities in housing markets.\n\nSpatial Features: - Economic Density (EconKDE) (β = -3,726.664, p &lt; 0.001): The linear term shows a negative coefficient, suggesting that at low levels of economic activity, proximity to commercial areas may initially depress housing values, possibly due to noise, traffic, or other negative externalities.\n\nEconomic Density² (EconKDE²) (β = 26,476,926, p &lt; 0.001): The positive quadratic term indicates that the relationship between economic density and housing prices is U-shaped. As economic activity increases beyond a certain threshold, the benefits of proximity to amenities, employment, and urban vibrancy begin to outweigh the costs, leading to premium pricing in highly commercial areas.\nMean 3-Nearest Neighbor Distance (β = 0.00002, p &lt; 0.001): The positive coefficient suggests that properties located farther from their three nearest neighbors command slightly higher prices, possibly reflecting larger lot sizes or more exclusive locations.\n\nIncome Quantile Effects: - MHI_quantileQ2 (β = 0.136, p &lt; 0.001): Properties in the second income quartile neighborhoods command approximately 14.6% higher prices than those in the lowest quartile, reflecting the premium associated with middle-income areas.\n\nAge-Income Interactions: The positive interaction terms between age and higher income quantiles (Q2: β = 0.001, Q3: β = 0.003, Q4: β = 0.004) suggest that older homes retain more value in wealthier neighborhoods, likely due to better maintenance, historic preservation, or neighborhood character that offsets age-related depreciation.\n\nThe economic density relationship is particularly noteworthy—it demonstrates that Philadelphia’s housing market exhibits a complex spatial pattern where moderate commercial activity may initially depress values, but high-density commercial areas (like Center City) command significant premiums, reflecting the urban amenities premium that characterizes successful city centers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "Project Overview:\n\nMembers: Jack Bader, Date: October 27, 2025"
  }
]