---
title: "Technical Appendix"
format: html
---

```{r}
#| echo: False
# Load packages
library(tidycensus)
library(tidyverse)
library(knitr)
library(scales)
library(RColorBrewer)
library(sf)
library(ggplot2)
library(tigris)
library(patchwork)
library(here)
library(osmextract) 
library(dplyr)
library(tigris)
library(spatstat.geom)
library(spatstat)
library(terra)
library(httr)
library(jsonlite)
library(FNN)
library(stargazer)
library(car)
library(lmtest)
options(tigris_use_cache = TRUE, tigris_progress = FALSE) 
```

# PHASE 1: DATA PREPARATION

## 1.1 Load and Philadelphia house sales data
```{r}
# Load Philly Property Sales data
phl_sales <- read_csv(here("data", "raw", "opa_properties_public.csv"))

```

### Filter to residential properties, 2023-2024 sales
```{r}
# Check data types
# glimpse(phl_sales)

phl_sales_res_23_24 <- phl_sales |>
  filter(
    category_code == 1, # Residential
    year(sale_date) %in% c(2023, 2024), # 2023-24 sales
    !is.na(category_code) & !is.na(sale_date) # Handle nulls
  )
```

### Remove obvious errors
```{r}
phl_sales_clean <- phl_sales_res_23_24 |>
  filter(
    # Some sale_price are unrealistically too low ($0, $1 etc.)
    sale_price >= 10000,
    # Exclude homes with 0 bathrooms
    number_of_bathrooms > 0,
    # Some areas are unrealistically low (0, 1, etc.)
    total_area > 1,
    # Some 0's remain in total_liveable_area after first area filter
    total_livable_area > 0,
    # Filter our unrealistic year built
    year_built >= 1750
    ) 
```

### Handle missing values

```{r}
# Check how many features have NA values
# sum(is.na(phl_sales_clean$number_of_bedrooms))
# sum(is.na(phl_sales_clean$number_of_bathrooms))
# sum(is.na(phl_sales_clean$total_livable_area))
# sum(is.na(phl_sales_clean$year_built))

# Remove the 2 observations with NA values for number of bedrooms
phl_sales_clean <- phl_sales_clean |>
  filter(
    !is.na(number_of_bedrooms)
  )
```

### Preliminary Feature Engineering: Age = sale date - year built
```{r}
phl_sales_clean <- phl_sales_clean |>
  mutate(
    sale_year = year(sale_date),
    age = sale_year - year_built
  )
```

### Document all cleaning decisions


- Our methodology for cleaning the Philadelphia home sales data is to focus on the features used in our model. As a group, we decided on the following independent variables to consider in our data exploration and model building to be: number of bathrooms, number of bedrooms, total livable area, and year built. We recognize that there is some risk of collinearity between these structural features, which will later be monitored and addressed if needed in the model building stage. Additionally, we also had to clean the sales price column since this is the variable we aim to predict in our model.


- Filter for only **residential properties** & **sales made in 2023-24** (per instructions).


- Filter for **realistic sales price** >= $10,000.


- Filter for houses with **at least 1 bathroom**. We will keep observations where **number of bedrooms = 0** as this likely signifies a studio apartment. However, it is not feasible for homes to have zero bathrooms, so we will enforce a constraint that a home must have at least 1 bathroom to preserve data integrity.


- Filter for **realistic total area** > 1 sq ft & **realistic total livable area** > 0 sq ft.


- Filter for **year built** >= 1750 (some homes were built in year 0).


- **Handle missing values:** We removed any missing values in our dependent variable of sales price, since it is crucial we have a true and accurate measure for prediction. We also checked which of our predictor variables had NA values after filtering. Only number of bedrooms had 2 remaining NA values. The rest had no NA values. To remedy this, we will remove the 2 observations from our data. Note, if there was substantial missing values in our predictors, we could use strategies such as imputing the NA values with the mean or median to use when building our model. 


- **Preliminary feature engineering:** Rather than using year built in our Automated Valuation Model, it makes more sense to create a new variable **age** that is equal to the sale date minus the year built. The **age** variable is often easier to interpret in exploratory plots with the newer houses appearing on the left and older ones on the right. This is primarily a stylistic preference: the overall pattern of the data will remain the same but mirrored.


## 1.2 Load Secondary Data

### Census

Purpose: Pull demographic and housing data at the census tract level for Philadelphia from the 2023 5-year ACS. This data will provide predictors for neighborhood characteristics in our modeling.

Variables Collected:

Median household income (B19013)

Percentage of family households (B11001)

Education attainment: percent of population 25+ with a bachelor’s degree or higher (B15003)

Housing vacancy rate (B25002)

Racial composition: percent white (B02001)

```{r api key, eval=FALSE, include=FALSE}

# This Block is the code that I used to Pull and Manipulate the Census Data. I condensed it and included it in one block for the team's reference. This output files have been uploaded to the project and will be pulled in the next code block to reduce computation.

census_api_key <- ("8deb926015351433bef092c55a5e8a90573283bd")


#philly-tract-data

# Define parameters
my_state  <- "Pennsylvania"
my_county <- "Philadelphia"
acs_year  <- 2023  # latest available 5-year ACS (2019–2023)

# Median Household income
income <- get_acs(
  geography = "tract",
  state = my_state,
  county = my_county,
  table = "B19013",
  year = acs_year,
  survey = "acs5"
) %>%
  select(GEOID, median_income = estimate)

# Percentage of family households
households <- get_acs(
  geography = "tract",
  state = my_state,
  county = my_county,
  table = "B11001",
  year = acs_year,
  survey = "acs5"
) %>%
  select(GEOID, variable, estimate) %>%
  tidyr::pivot_wider(names_from = variable, values_from = estimate) %>%
  mutate(
    family_hh = B11001_002,
    total_hh  = B11001_001
  ) %>%
  select(GEOID, family_hh, total_hh)

# Education (percent bachelor’s or higher)
education <- get_acs(
  geography = "tract",
  state = my_state,
  county = my_county,
  table = "B15003",
  year = acs_year,
  survey = "acs5") %>%
  select(GEOID, variable, estimate) %>%
  tidyr::pivot_wider(names_from = variable, values_from = estimate) %>%
  mutate(
    total_25plus = B15003_001,
    bachelors_plus = B15003_022 + B15003_023 + B15003_024 + B15003_025,
    pct_bachelors = 100 * bachelors_plus / total_25plus
  ) %>%
  select(GEOID, pct_bachelors)

# Percent of vacant homes = number vacant/total homes
vacancy <- get_acs(
  geography = "tract",
  state = my_state,
  county = my_county,
  table = "B25002",
  year = acs_year,
  survey = "acs5"
) %>%
  select(GEOID, variable, estimate) %>%
  tidyr::pivot_wider(names_from = variable, values_from = estimate) %>%
  mutate(
    total_units = B25002_001,
    vacant_units = B25002_003,
    pct_vacant = 100 * vacant_units / total_units
  ) %>%
  select(GEOID, pct_vacant)

# retrieving percent white for census tract
demographics <- get_acs(
  geography = "tract",
  state = my_state,
  county = my_county,
  table = "B02001",
  year = acs_year,
  survey = "acs5") %>%
  select(GEOID, variable, estimate) %>%
  tidyr::pivot_wider(names_from = variable, values_from = estimate) %>%
  mutate(
    total_pop = B02001_001,
    white_pop = B02001_002,
    pct_white = 100 * white_pop / total_pop
  ) %>%
  select(GEOID, pct_white)

philly_censustract <- income %>%
  left_join(households, by = "GEOID") %>%
  left_join(education, by = "GEOID") %>%
  left_join(vacancy, by = "GEOID") %>%
  left_join(demographics, by = "GEOID")

head(philly_censustract)

#census tract Geometry Pull
philly_tract_sf <- get_acs(
  geography = "tract",
  state = "PA",
  county = "Philadelphia",
  table = "B19013",
  year = 2023,
  survey = "acs5",
  geometry = TRUE)

head(philly_tract_sf)

# Geometry - Cenus Data Merge
philly_tract_map <- philly_tract_sf %>%
  left_join(philly_censustract, by = "GEOID")
```

#### Load Philly Census Data from Previously Retrieved Files

```{r}

# Relative to project root
census_path <- here("data", "Philly Census")

census_csv_path <- file.path(census_path, "philly_tract_metrics.csv")
census_shp_path <- file.path(census_path, "philly_tract.shp")

# Csv with census tract Geo IDs and metrics
philly_censustract <- read_csv(census_csv_path, show_col_types = FALSE)

# Shp File including geometry
philly_tract_sf <- st_read(census_shp_path, quiet = TRUE)
```

#### Observe Summary Statistics from target metrics.x

```{r}
numeric_vars <- c("median_income", "pct_white", "pct_bachelors", "pct_vacant")
philly_censustract %>%
  select(all_of(numeric_vars)) %>%
  summary()
```
A quick check of the census variables reveals some missing values and lower than epected values in median income. We will note this information but retain the missing values for now to maintain the full pitcure of census blocks.


### Cleaning Methodology (Census)

Median income: Selected only the estimate column and renamed it for clarity.

Household composition: Pivoted ACS table to wide format, then calculated total households and family households.

Education: Pivoted to wide format, summed relevant categories to compute percent of population with a bachelor’s degree or higher.

Vacancy: Pivoted to wide format, calculated percent of homes vacant (vacant_units / total_units * 100).

Racial composition: Pivoted to wide format, computed percent white.

Merging: Combined all datasets by GEOID to create a single dataframe philly_blockgroup with all variables.

Geometry: Pulled census tract shapefiles with ACS geometry and merged with philly_blockgroup to create philly_bg_map.

### Neighborhood (Polygon)

Reading in Philadelphia Neighborhoods as a shp object. This will allow us to aggregate data on neighborhoods to identify catagorical metrics.

```{r}
neighborhood_folder <- here("data", "philadelphia-neighborhoods")
neighborhood_path   <- file.path(neighborhood_folder, "philadelphia-neighborhoods.shp")

# Read the shapefile
philly_neighborhoods <- st_read(neighborhood_path, quiet = TRUE)

head(philly_neighborhoods)
```


### Commercial and office points of interests (Amenities)

```{r}
# downloading osm data from geofabrik:https://download.geofabrik.de/north-america/us-northeast.html
input_pbf <- "C:/Users/matth/Documents/pennFall25/PPA/midterm/us-northeast-251024.osm.pbf"


# get boundary of Philadelphia County
pa_counties <- counties(state = "PA", year = 2023)

# Filter to Philadelphia County
philly_boundary <- subset(pa_counties, NAME == "Philadelphia")

# read the full OSM PBF (you can select layer types like points, lines, polygons)
poi <- oe_read(input_pbf, 
                       boundary = philly_boundary, 
                       boundary_type = "clipsrc", 
                       layer = "points")  # or "lines" / "multipolygons"


keywords <- c("shop","amenity","office","historic","tourism","healthcare",
              "building","leisure")
pattern <- paste0(keywords, collapse = "|")

# ==== Filter by 'other_tags' ====
if ("other_tags" %in% names(poi)) {
  poi$other_tags <- iconv(as.character(poi$other_tags), from = "", to = "UTF-8", sub = "")
  poi$other_tags[is.na(poi$other_tags)] <- ""
  
  poi_filtered <- poi %>%
    filter(grepl(pattern, other_tags, ignore.case = TRUE))
  
  cat("✅ Filtered POIs found:", nrow(poi_filtered), "of", nrow(poi), "\n")
  
} 


```

###Alternative: filtered POI if you donot want to download osm data
```{r}

poi_path <- here("data", "filtered poi")
poi_shp_path=file.path(poi_path, "philadelphia_poi_filtered.shp")
poi=st_read(poi_shp_path)
```

### Kernel Density Rasters (Economic activities density)
Instead of using distance to CBD, we extracted commercial and office points of interests from OpenStreetMap (OSM), and we opreate a Kernel Density Estimation (KDE) with a bandwidth of 300 meters. By doing that, we manage to get a surface of density of economic activities across the whole city. The higher the KDE value is, the more economic activities it will be, implying a higher likelyhood of the area as a city centers.

There are several benefits using this approach compared to distance to CBD. First, with the development of suburbanization, even within the context of Philadelphia County, there is still a shift from monocentric model to polycentric model, meaning there multiple centers/subcenters. Using one CBD fail to capture these subcenters, which may also influence housing price. Second, CBD is an area rather than a point, distance method fail to capture this while the continuous surface computed by KDE would have a value of economic activities across the whole city. 

```{r}
philly_boundary <- st_transform(philly_boundary, 3364)  
poi <- st_transform(poi, 3364)
# ==== Prepare point pattern ====
# Convert sf points to spatstat ppp object
win <- as.owin(st_union(philly_boundary))  # window from county boundary
coords <- st_coordinates(poi)
pp <- ppp(x = coords[,1], y = coords[,2], window = win)

# ==== Run Kernel Density Estimation ====
# Sigma = bandwidth in map units (here, meters)
density_map <- density.ppp(pp, sigma = 300* 3.28084, edge = TRUE, at = "pixels",eps = c(100, 100))

# ==== Convert to raster ====
r_Economic <- rast(density_map)
crs(r_Economic) <- st_crs(philly_boundary)$proj4string
r_Economic <- mask(r_Economic, vect(philly_boundary))

```

### Education

We used two datasets from OpenDataPhilly.com to identify schools geolocation and populated the metrics off Attendance percent and Withdrawal volumes from those schools.

```{r}
# Relative to project root
education_path <- here("data", "Education")

education_csv_path <- file.path(education_path, "philadelphia_schools.csv")
education_shp_path <- file.path(education_path, "Schools Shape", "Schools.shp")

# Csv with School Names and metrics
philly_schools <- read_csv(education_csv_path, show_col_types = FALSE)

# Shp File including geometry
philly_schools_sf <- st_read(education_shp_path, quiet = TRUE)
```

We joined the csv file containing the metrics with the shp file containing geoloaction.

```{r}
# Joining Schools csv metrics to shp file. Joined on 'location_i' (shp) and 'School_code (csv)
# Keeping metrics for Attendance and Withdrawals

# Select relevant metrics from CSV
school_metrics <- philly_schools %>%
  select(School_code, Attendance, Withdrawals) %>%
  mutate(School_code = as.character(School_code))

philly_schools_sf <- philly_schools_sf %>%
  left_join(school_metrics,
            by = c("location_i" = "School_code"))
```
```{r}
philly_schools_sf_clean <- philly_schools_sf %>%
  filter(!is.na(Attendance) & !is.na(Withdrawals))
```
```{r}
names(philly_schools_sf_clean)
nrow(philly_schools_sf_clean)
```
Once joined, we dropped rows that did not have values in Attendance and Withdrawal. This resulted in 204 public schools and their metrics located in Philadelphia City Limits.

###Tree density
Location of trees data was extracted from Opendata Philly. A Kernel Density Estimation was used to estimate the density of trees. The higher the value is, the more trees there will be in this (and surronding) cell
```{r}
tree_path <- here("data", "ppr_tree_inventory_2024")

tree_shp_path <- file.path(tree_path, "ppr_tree_inventory_2024.shp")


trees=st_read(tree_shp_path)
trees=st_transform(trees,3364)

# Convert sf points to spatstat ppp object
coords_trees <- st_coordinates(trees)
pp_trees <- ppp(x = coords_trees[,1], y = coords_trees[,2], window = win)

# ==== Run Kernel Density Estimation ====
# Sigma = bandwidth in map units (here, meters)
density_map_trees <- density.ppp(pp_trees, edge = TRUE, at = "pixels",eps = c(100, 100))

# ==== Convert to raster ====
r_trees <- rast(density_map_trees)
crs(r_trees) <- st_crs(philly_boundary)$proj4string
r_trees <- mask(r_trees, vect(philly_boundary))  # mask to county boundary

```

### Joining data together
As our housing data is point, while census tract and neighborhood are polygon. A spatial join with within as the method was performed to merge them. For raster, the value of the cell that the point locate was joined. 
```{r}
#convert housing prices data into point data
phl_sales_clean_sf = phl_sales_clean%>%
  mutate(geometry = st_as_sfc(shape)) %>%   # parse WKT into geometry
  st_as_sf(crs = 2272)  
#convert and match the crs
philly_schools_sf_clean=philly_schools_sf_clean%>%
  st_transform(2272)
philly_neighborhoods=philly_neighborhoods%>%
  st_transform(2272)
philly_tract_map=philly_tract_map%>%
  st_transform(2272)
#merge them together
phl_sales_clean_sf_final=phl_sales_clean_sf%>%
  st_join(philly_tract_map,join=st_within)%>%
  st_join(philly_neighborhoods,join = st_within)
phl_sales_clean_sf_final <- st_transform(phl_sales_clean_sf_final, crs = st_crs(r_trees))
phl_sales_clean_sf_final$EconKDE <- raster::extract(r_Economic,phl_sales_clean_sf_final)
phl_sales_clean_sf_final$TreeKDE <- raster::extract(r_trees, phl_sales_clean_sf_final)  
phl_sales_clean_sf_final=phl_sales_clean_sf_final%>%
  st_transform(2272)
```

# PHASE 2: EXPLORATORY DATA ANALYSIS

### Distribution of sale prices (histogram)

```{r}
# Calculate the median / mean to plot
price_median <- median(phl_sales_clean$sale_price, na.rm = TRUE)

ggplot(phl_sales_clean, aes(sale_price)) +
  geom_histogram(bins = 60, fill = "darkseagreen3", color = "black") +
  geom_vline(xintercept = price_median, linetype = 5) +
  annotate("text",
           x = price_median, 
           y = 6300, 
           label = "Median",
           hjust = -0.2, 
           color = "black", 
           size = 3) +
  scale_x_continuous(labels = label_dollar()) +
  labs(title = "Distribution of Home Sale Prices",
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Sale Price",
       y = "Count") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** The histogram plot above shows the full distribution of home sale prices for residential properties in 2023-2024 from our cleaned dataset. The data is extremely right-skewed, highlighting a majority of prices under \$500,000 with a long tail of more expensive homes thereafter. The main issue with the extreme outliers of home prices exceeding \$5 million that make the visibility of this plot hard to interpret. Also, there are major gaps at higher sales prices as we see the data become more spread out, further indicating the presence of outliers. Therefore, to remedy this, we will plot a second histogram of the price distribution, excluding the top 5% of sales prices from the original cleaned dataset.

```{r}
# Create new df, filtering out the top 5% of house prices (outliers)
price_95_perc <- quantile(phl_sales_clean$sale_price, 0.95, na.rm = TRUE)
df_95_exclude <- filter(phl_sales_clean, sale_price <= price_95_perc)

# Calculate the median / mean to plot from trimmed distribution
price_median_95_exc <- median(df_95_exclude$sale_price, na.rm = TRUE)
price_mean_95_exc <- mean(df_95_exclude$sale_price, na.rm = TRUE)


ggplot(df_95_exclude, aes(sale_price)) +
  geom_histogram(bins = 20, fill = "darkseagreen3", color = "black") +
  geom_vline(xintercept = price_mean_95_exc, linetype = 5) +
  geom_vline(xintercept = price_median_95_exc, linetype = 5) +
  annotate("text",
           x = price_mean_95_exc,
           y = 2400,
           label = "Mean", 
           hjust = - 0.1, 
           color = "black", 
           size = 3) +
  annotate("text",
           x = price_median_95_exc, 
           y = 2450, 
           label = "Median",
           hjust = 1.25, 
           color = "black", 
           size = 3) +
  scale_x_continuous(labels = label_dollar()) +
  labs(
    title = "Distribution of Home Sale Prices",
    subtitle = "For Philadelphia Residential Properties in 2023-2024",
    caption = "Histogram and median/mean statistics were computed on filtered sample (sale price ≤ 95th percentile) for better visibility.",
    x = "Sale Price",
    y = "Count") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  ) 
```

**Interpretation:** In this revised histogram, we get a much better sense of how the sales prices are distributed without the presence of extreme outliers. As we saw before, the data is definitely right-skewed since the median (\$235,250) is less than the mean (\$322,231) even when removing large outliers from the top 5% of the distribution. We can visualize that the standard housing market in Philadelphia from 2023-2024 ranges between \$0 and \$800,000. The distribution has a single peak around \$200,000, indicating that it is unimodal with a typical (or most common) home sale price in the realm of \$150,000 to \$250,000. This is an indication that it may be best to omit these significant outliers from our dataset when building our model for home sale price prediction. The homes on the higher end of the sale price distribution are determined by a combination of structural features (such total livable area) and spatial features (such as nearby city centers) that drive up the prices of these homes. The goal of this study is to determine what features, both structural and spatial, are significant in predicting home sale prices in Philadelphia and create an accurate model to help policy makers in valuating property tax assessments.

### Geographic distribution of sales price by tract

```{r}
library(tmap)

price_95_perc <- quantile(phl_sales_clean$sale_price, 0.95, na.rm = TRUE)
df_95_exclude <- phl_sales_clean %>%
  filter(sale_price <= price_95_perc)

df_95_exclude_sf <- df_95_exclude %>%
  mutate(geometry = st_as_sfc(shape)) %>%
  st_as_sf(crs = 2272)

df_95_exclude_sf <- df_95_exclude_sf %>%
  st_join(philly_tract_map, join = st_within)

tract_price_summary <- df_95_exclude_sf %>%
  st_drop_geometry() %>%
  group_by(GEOID) %>%
  summarize(median_price = median(sale_price, na.rm = TRUE))

philly_price_map <- philly_tract_map %>%
  left_join(tract_price_summary, by = "GEOID")

tmap_mode("plot")

tm_shape(philly_price_map) +
  tm_polygons(
    col = "median_price",
    style = "quantile",
    n = 5,
    palette = "YlGnBu",
    title = "Median Sale Price (≤ 95th Percentile)"
  ) +
  tm_layout(
    title = "Median Home Sale Prices by Census Tract (Filtered to ≤ 95th Percentile)",
    legend.outside = TRUE
  ) +
  tm_shape(df_95_exclude_sf) +
  tm_dots(
    col = "gray30",
    alpha = 0.3,
    size = 0.02,
    legend.show = FALSE
  )
```

**Interpretation:**

This visualization shows clear spatial clustering of home prices in Philadelphia when looking at the typical market excluding outliers. Higher median prices are concentrated in Center City, University City, and select neighborhoods in South Philadelphia and Northwest Philadelphia. This supports our need to include additional predictors to account amenity density and economic activity.

### Distribution of median income by neighborhood

```{r}
census_points <- st_centroid(philly_tract_map)
points_joined <- st_join(census_points, philly_neighborhoods)

mean_income_by_neighborhood <- points_joined %>%
  st_drop_geometry() %>%
  group_by(MAPNAME) %>%
  summarise(mean_income = mean(median_income, na.rm = TRUE))

philly_neighborhood_income <- philly_neighborhoods %>%
  left_join(mean_income_by_neighborhood, by = "MAPNAME")

# 4. Plot choropleth
tmap_mode("plot")

tm_shape(philly_neighborhood_income) +
  tm_polygons(
    col = "mean_income",
    style = "quantile",
    n = 5,
    palette = "YlOrRd",
    title = "Median Household Income"
  ) +
  tm_layout(
    title = "Median Household Income by Neighborhood",
    legend.outside = TRUE
  )
```
**Interpretation:**

In this visualization depicting median incomes of residents by neighborhoods, we observe continued spatial clustering of wealth indicators in areas like Center City, Northwest Philadelphia, and South Philadlephia. This is consistent with the basic aggregation of home sale prices on census tracts. The presence of high income in tandem with high housing cost serves as an indication of their relationship and justifies the use of economic predictors in our model. With this spatial auto-correlation, it is important that the model absorbs spatial variation by utilizing additional spatial features.   

### Price vs. structural features

#### 1. Number of Bathrooms

```{r}
ggplot(phl_sales_clean, aes(x = factor(number_of_bathrooms), y = sale_price)) +
  geom_boxplot(fill = "lightcyan2", outlier.color = "firebrick", outlier.alpha = 0.2) +
  scale_y_continuous(labels = scales::label_dollar()) +
  labs(title = "Distribution of Home Sale Prices by Number of Bathrooms", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Bathrooms", 
       y = "Sale Price") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** Since number of bathrooms is a discrete variable, a scatter plot is not suitable to visualize this predictor's relationship with the target variable of sale price; therefore, we opted to use box plots instead. In this first plot, we see a distinct positive relationship between number of bathrooms and home sale price. Intuitively, this makes sense since homes with more bathrooms should on average sell at higher prices. Another trend is that there are more outliers in homes with less bathrooms (between 1 and 3). These outliers are likely due to external spatial factors such as neighborhoods. Housing prices tend to surge in highly desirable neighborhoods such as Rittenhouse Square, which explains the presence of many outliers plotted above the upper bound of these boxplots. Another trend is that variance of sale prices begins to significantly widen for homes with more than 3 bedrooms. This suggests that larger homes with more bathrooms experience more price dispersion relative to those with less bathrooms. Lastly, it is worth noting that there are very few observations of homes with 8 or 12 bathrooms, indicating that it would be beneficial to remove them from our dataset to better capture the true relationship between price and number of bathrooms.

```{r}
ggplot(phl_sales_clean, aes(x = factor(number_of_bathrooms), y = log(sale_price))) +
  geom_boxplot(fill = "lightcyan2", outlier.color = "firebrick", outlier.alpha = 0.2) +
  scale_y_continuous(labels = scales::label_dollar()) +
  labs(title = "Distribution of Log(Home Sale Prices) by Number of Bathrooms", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Bathrooms", 
       y = "Log(Sale Price)") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** In addition, we also plotted with the y-axis transformed to the log of sales price. We notice that there is still a positive relationship that appears more linear than the first plot. The log transformation helps reduce the effect of extreme outliers by compressing the distribution of house sale prices. This transformation also serves to center the distributions as noted by the presence of both positive and negative outliers. These benefits suggest that log-transforming our target variable for a linear regression is the best suitable method to yield an accurate model. 

#### 2. Number of Bedrooms

```{r}
ggplot(phl_sales_clean, aes(x = factor(number_of_bedrooms), y = sale_price)) +
  geom_boxplot(fill = "lightcyan2", outlier.color = "firebrick", outlier.alpha = 0.2) +
  scale_y_continuous(labels = scales::label_dollar()) +
  labs(title = "Distribution of Home Sale Prices by Number of Bedrooms", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Bedrooms", 
       y = "Sale Price") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** The above plot to measure the relationship between home sale price and number of bedrooms is positive, indicating that homes with more bedrooms tend to sell at higher prices. However, this relationship looks significantly less strong than the relationship between sale price and number of bathrooms. We observe many outliers across homes with various bedroom sizes, most notably those with 2 - 5 bedrooms. The intuition is that these size households are more likely to be sold on the market with higher variability in sales prices, particularly in the upper tail, demonstrating right-skew. These high outliers represent luxury properties that sell for abnormally high sale prices due to other external variables like neighborhood, size etc. Again, we have a small number of very large homes with 10, 11, and 12 bedrooms that are candidates for removal before building our model.

```{r}
ggplot(phl_sales_clean, aes(x = factor(number_of_bedrooms), y = log(sale_price))) +
  geom_boxplot(fill = "lightcyan2", outlier.color = "firebrick", outlier.alpha = 0.2) +
  scale_y_continuous(labels = scales::label_dollar()) +
  labs(title = "Distribution of Log(Home Sale Prices) by Number of Bedrooms", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Bedrooms", 
       y = "Log(Sale Price)") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** As before, we can log-transform the target variable log(sale price) to better visualize the relationship with number of bedrooms. We still see a positive relationship that appears more linear than in our earlier plot. As before, the log-transformation stabilizes the variance to mitigate outliers and centers the data demonstrated by outliers at both the upper and lower tails. The strength of the relationship still appears to be not as obvious as with the number of bathrooms. In other words, number of bathrooms may be a more suitable predictor of sale price from these EDA observations. To avoid the risk of multicollinearity, we will keep this finding in mind when determining what structural features should be included in our final model.

#### 3. Total Livable Area

```{r}
ggplot(phl_sales_clean, aes(x = total_livable_area, sale_price)) +
  geom_point(alpha = 0.1, size = 1) +
  geom_smooth(method = "lm", se = FALSE, linetype = 5, color = "firebrick") +
  scale_y_continuous(labels = label_dollar()) +
  labs(title = "Distribution of Home Sale Prices by Total Livable Area", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Total Livable Area", 
       y = "Sale Price") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** The plot represents the relationship between the non-transformed target sale price and predictor total livable area. We can see that the relationship to be positive with evidence of heavy right-skew with the majority of data points clustered in the bottom-left with less than 3000 sq ft of area and \$500,000 price. Also, we notice that although the relationship is positive, it does not appear to be linear, a violation of a crucial assumption in linear regression. As before, there is the presence of luxury homes as large outliers that can pull the regression line upward, which would create biased estimates on model coefficients.

```{r}
ggplot(phl_sales_clean, aes(x = log(total_livable_area), log(sale_price))) +
  geom_point(alpha = 0.075, size = 1) +
  geom_smooth(method = "lm", se = FALSE, linetype = 5, color = "firebrick") +
  scale_y_continuous(labels = label_dollar()) +
  labs(title = "Distribution of Log(Home Sale Prices) by Total Livable Area", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Log(Total Livable Area)", 
       y = "Log(Sale Price)") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** By log-transforming both sale price and total livable area, we visualize a relationship that appears much more linear. As we saw before, there seem to be more variability in homes with smaller areas that are more common in this dataset. The transformation has more of a uniform, symmetric spread of points above and below the regression line. We can interpret this relationship as increasing the percentage of total livable area will lead to some constant increase in the percentage of sale price with this transformation. We should keep at mind that the non-constant variance is alarming for potential heteroskedasticity in our model. Again, through the log-transformation of our target variable, we observe a more linear relationship, strengthening the notion for the need for this transformation in our modeling phase.

#### 4. Age (Sale Date - Year Built)

```{r}
ggplot(phl_sales_clean, aes(x = age, y = sale_price)) +
  geom_point(alpha = 0.076, size = 1) +
  scale_y_continuous(labels = label_dollar()) +
  labs(title = "Distribution of Home Sale Prices by Age", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Age", 
       y = "Sale Price") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** In the above plot, it does not reveal any significant relationship between sale price and home age. The main issue is that most of the data is clustered at the bottom of the figure at low sale prices regardless of age. There are some noticeable outliers along with increased variability for middle-aged homes around 75 to 125 years old. In general, it seems that there newer homes (lower age) demonstrate somewhat higher price levels on average. However, from this plot alone, it is difficult to verify.

```{r}
ggplot(phl_sales_clean, aes(x = age, log(sale_price))) +
  geom_point(alpha = 0.075, size = 1) +
  geom_smooth(method = "loess", span = 0.75, se = FALSE, color = "firebrick") +
  scale_y_continuous(labels = label_dollar()) +
  labs(title = "Distribution of Log(Home Sale Prices) by Age", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "Age", 
       y = "Log(Sale Price)") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** After doing the trick to log-transform sale price, we see an interesting shape that somewhat resembles a crucial theory we learned in lecture. This theory outlines that Age has a U-shaped effect on price. Newer homes are sought after due to recent construction with more modern amenities, whereas very old homes are considered to have historic value or charm. The middle-aged homes are therefore the least desirable homes that likely are not modernized with significant wear and tear without the allure of a historic home. While this theory makes sense, it somewhat breaks down for these middle aged homes as their sale prices experience much more variability, which would make our model more at risk of heteroskedasticity. This is likely due to a majority of samples in our dataset within this range whose sale price cannot be explained by age alone. Therefore, it is wise to proceed with caution with using the age variable in building our predictive model.

```{r}
df_age_groups <- phl_sales_clean |>
  mutate(
    age_group = case_when(
      age < 20 ~ "New (<20 years)",
      age < 80 ~ "Middle (20–80 years)",
      age >= 80 ~ "Historic (>80 years)"
    ),
    age_group = factor(age_group, levels = c("New (<20 years)", "Middle (20–80 years)", "Historic (>80 years)"))
  )

ggplot(df_age_groups, aes(age_group, log(sale_price))) +
  geom_boxplot(fill = "lightcyan2", outlier.colour = "firebrick", outlier.alpha = 0.2) +
  labs(
    title = "Distribution of Log(Sale Price) by Age Group",
    subtitle = "Philadelphia Residential Sales in 2023–2024",
    x = "Age Group", y = "log(Sale Price)",
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )
```

**Interpretation:** When we engineer a new age group feature, the U-shaped distribution does not appear to be valid in the case of our dataset. Using the age segmentations defined in class, it now appears as there is an inverse (negative) relationship between log(sale price) and age group, not a U-shape distribution. One hypothesis is that very old buildings in Philadelphia are too worn down or don't hold enough historic value to counteract its age. Whatever the reason may be, it seems that engineering a feature for age group is preferred over using age directly and to expect increasing age to a home to depreciate its value over time.

### Price vs. spatial features

```{r}
ggplot(phl_sales_clean_sf_final, aes(x = EconKDE$lyr.1, y=sale_price)) +
  geom_point(alpha = 0.1, size = 1) +
  geom_smooth(method = "lm", se = FALSE, linetype = 5, color = "firebrick") +
  scale_y_continuous(labels = label_dollar()) +
  labs(title = "Distribution of Home Sale Prices by Economic KDE Value", 
       subtitle = "For Philadelphia Residential Properties in 2023-2024",
       x = "KDE", 
       y = "Sale Price") +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )

```

**Interpretation:**
The plot represents the relationship between the non-transformed target sale price and predictor KDE for economic activities. We can see that the relationship to be positive with evidence of heavy right-skew with the majority of data points clustered in the bottom-left with less than a KDE value of 0.000075 and \$500,000 price. Also, we notice that although the relationship is positive, it does not appear to be linear, a violation of a crucial assumption in linear regression. It seems that the trend increases first till about 0.00008 and then decreases. As before, there is the presence of some homes as large outliers that can pull the regression line upward, which would create biased estimates on model coefficients.

### One creative visualization

```{r}

```

**Interpretation:**


# PHASE 3: FEATURE ENGINEERING

### Final Data Cleaning from Structural EDA

```{r}
# 99th percentile of sales prices to remove
price_99_perc <- quantile(phl_sales_clean$sale_price, 0.99, na.rm = TRUE)

# Relevant columns to keep for modeling
rel_columns <- c(
    # Location / Shape
    "census_tract", "shape", "location", "zip_code",
    # Target
    "sale_price", "log_price",
    # Regressors (and potential ones)
    "total_livable_area", "log_total_livable_area",
    "number_of_bedrooms", "number_of_bathrooms",
    "year_built", "age", "age_group",
    "interior_condition", "quality_grade" 
  )

phl_sales_final <- phl_sales_clean |>
  filter(
    # Remove top 1% of sale price (extreme outliers)
    # This creates an upper bound of around $2 million rather than $6 million to further reduce outlier effects in modeling
    sale_price < price_99_perc,
    # Remove bathrooms > 7
    number_of_bathrooms < 8,
    # Remove bedrooms > 9
    number_of_bedrooms < 10
  )|>
  mutate(
    # Log of sale price
    log_price = log(sale_price),
    # Log of total livable area
    log_total_livable_area = log(total_livable_area),
    # Age group buckets (new, middle_age, historic)
    age_group = case_when(
      age < 20 ~ "New (<20)",
      age <= 80 ~ "Middle (20–80)",
      age > 80 ~ "Historic (>80)"
    ),
    # For modeling as dummy variables
    age_group = factor(age_group, levels = c("New (<20)", "Middle (20–80)", "Historic (>80)"))
  ) |>
  # Select relevant columns
  select(any_of(rel_columns))


#convert housing prices data into point data
phl_sales_final_sf = phl_sales_final%>%
  mutate(geometry = st_as_sfc(shape)) %>%   # parse WKT into geometry
  st_as_sf(crs = 2272)  
#convert and match the crs
philly_schools_sf_final=philly_schools_sf_clean%>%
  st_transform(2272)
philly_neighborhoods=philly_neighborhoods%>%
  st_transform(2272)
philly_tract_map=philly_tract_map%>%
  st_transform(2272)
#merge them together
phl_sales_final_sf_final=phl_sales_final_sf%>%
  st_join(philly_tract_map,join=st_within)%>%
  st_join(philly_neighborhoods,join = st_within)
phl_sales_final_sf_final <- st_transform(phl_sales_final_sf_final, crs = st_crs(r_trees))
phl_sales_final_sf_final$EconKDE <- raster::extract(r_Economic,phl_sales_final_sf_final)
phl_sales_final_sf_final$TreeKDE <- raster::extract(r_trees, phl_sales_final_sf_final)  
phl_sales_final_sf_final=phl_sales_final_sf_final%>%
  st_transform(2272)
```


### Classifying Neighborhood

```{r}
#convert census tract into points
census_points=st_centroid(philly_tract_map)
points_joined <- st_join(census_points, philly_neighborhoods)

# Calculate mean MHI for each polygon
mean_mhi_by_poly <- points_joined %>%
  st_drop_geometry() %>%             
  group_by(MAPNAME) %>%            
  summarise(meanMHI = mean(median_income, na.rm = TRUE))

# Join the result back to the polygon layer
phl_sales_final_sf_final <- phl_sales_final_sf_final %>%
  left_join(mean_mhi_by_poly, by = "MAPNAME")%>%
  st_as_sf() 
# reclassify the neighborhood data based on quantile (25%)
phl_sales_final_sf_final$MHI_quantile <- cut(
  phl_sales_final_sf_final$meanMHI,
  breaks = quantile(
    phl_sales_final_sf_final$meanMHI,
    probs = seq(0, 1, 0.25),   
    na.rm = TRUE
  ),
  include.lowest = TRUE,
  labels = c("Q1 (lowest)", "Q2", "Q3", "Q4 (highest)")
)
```

### Calculating distance to schools using K-nearest method

```{r}
# Compute distance matrix
dist_matrix <- st_distance(phl_sales_final_sf_final, philly_schools_sf_final)

# Extract the 3 smallest distances for each point
nearest_3 <- apply(dist_matrix, 1, function(x) sort(x)[1:3])

# Get the average or total distance if needed
mean_nearest3 <- apply(dist_matrix, 1, function(x) mean(sort(x)[1:3]))

phl_sales_final_sf_final$mean_3nn_dist <- mean_nearest3



```


### Summary table

```{r}
#reformat raster data into a "readable" format by stargazer
phl_sales_final_sf_final <- phl_sales_final_sf_final %>%
  mutate(EconKDE = EconKDE$lyr.1,
         TreeKDE = TreeKDE$lyr.1) 

#exclude the unnecessary data
summary_data <- phl_sales_final_sf_final %>%
  st_drop_geometry() %>%
  select(-c(
    census_tract, shape, NAME.x, NAME.y, MAPNAME,
    Shape_Area, Shape_Leng, location, zip_code, GEOID, 
    variable, age_group, quality_grade, LISTNAME, MHI_quantile,
    estimate,moe
  ))

# Keep only numeric columns
summary_numeric <- summary_data %>%
  select(where(is.numeric))
summary_numeric_df <- as.data.frame(summary_numeric)

# Save as HTML locally
stargazer(summary_numeric_df,
          type = "html",
          title = "Summary Statistics",
          digits = 2,
          summary.stat = c("n", "mean", "sd", "min", "median", "max"),
          covariate.labels = c("Sale Price", "Log Price", "Total Livable Area", 
                            "Log Total Livable Area", "Bedrooms", "Bathrooms", 
                            "Year Built", "Age", "Interior Condition", 
                            "Median Income","Family HH", 
                            "Total HH", "% Bachelors", "% Vacant", "% White",
                            "Econ KDE", "Tree KDE", "Mean MHI", 
                            "Mean 3 closest School Dist"),
          out = "summary_statistics.html")


# Print as txt in R
stargazer(summary_numeric_df,
          type = "text",
          title = "Summary Statistics",
          digits = 2,
          summary.stat = c("n", "mean", "sd", "min", "median", "max"),
          covariate.labels = c("Sale Price", "Log Price", "Total Livable Area", 
                            "Log Total Livable Area", "Bedrooms", "Bathrooms", 
                            "Year Built", "Age", "Interior Condition", 
                            "Median Income","Family HH", 
                            "Total HH", "% Bachelors", "% Vacant", "% White",
                            "Econ KDE", "Tree KDE", "Mean MHI", 
                            "Mean 3 closest School Dist"))
```
### Final Cleaning Before Building the Model

```{r}
for_model_building <- phl_sales_final_sf_final %>%
  st_drop_geometry() %>%
  select(-c(
    census_tract, shape, NAME.x, NAME.y,
    Shape_Area, Shape_Leng, location, zip_code, GEOID, 
    variable, LISTNAME, 
    estimate,moe
  ))
```



### Justification of feature engineered variables
We have used a number of methods to create spatial features in both Phase 1 and 3. First, in Phase 1, since commercial and office POI and trees are point data, and the more they cluster the higher the housing price will be. As a result, we use Kernel Density method to estimate the density of them. Value of the cell was assign to the point (housing prices), if the point falls within it. 

Second, as census tracts and neighborhood are polygon data, a st_within spatial join was used to join them with housing data. Values from polygon data will assign to the points when points fall within it. 

Third, as there are too many neighborhoods, it is hard to interpret the coefficient of them as it is a categorical variable. We reclassify it with four different categories based on quantile of MHI(25%). To calculate the MHI each neighborhood, we first take the centroid of each census tract and summarize them within each neighborhood. Then we take the mean of MHI of the tracts that fall within the neighborhood.  

Lastly, different from poi and trees, a high cluster of schools may not reflect a high housing price. As a result, instead of using density estimation, we use 3-nearest distance and take the mean to capture some of the closest schools.


#Phase 4: Model Building

####create train/test data first (make workflow smoother)
```{r}
set.seed(123)
n <- nrow(for_model_building)

# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- for_model_building[train_indices, ]
test_data <- for_model_building[-train_indices, ]
```

###Building the model progressively
####Model 1: Individual characteristic only
```{r}
#run regression based on houses own characteristic
options(scipen = 999)

model1=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
          +interior_condition+quality_grade,data=for_model_building)
summary(model1)

#check colinearity
vif(model1)
#vif looks fine, check heteroskedasticity

bptest(model1)
#p-value is way too small, implying more variables may be needed (make sense)

#calculate the RMSE
model_train_1 <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age+interior_condition+quality_grade,data = train_data)
test_predictions_1 <- predict(model_train_1, newdata = test_data)
rmse_test_1 <- sqrt(mean((test_data$log_price - test_predictions_1)^2,na.rm = TRUE))
rmse_train_1 <- summary(model_train_1)$sigma


plot(model1)
```
In residual plot, more observations are clustered when fitted value is lower, indicating bad distribution of residuals and nonlinearity. 

Same issue is reflected in Q-Q plot as well, the unlinearity revels the distribution of residual is not normal.

There is also heteroskedasticity, as the scale location shows, the line is not linear at all. 

The outliers seems to have small influence on the leverage,as the line is almost linear shown in the last graph



####Model 2: add census variables into regression
```{r}
options(scipen = 999)

model2=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
          +interior_condition+median_income+I(family_hh/total_hh)
          +pct_bachelors+pct_vacant+pct_white+quality_grade ,data=for_model_building)

summary(model2)
#number of bedroom becomes statistical insignificant after controling census variable

#check colinearity
vif(model2)
#edu_attainment and mhi may have muticolinearity

bptest(model2)
#p-value is way too small, implying more variables may be needed (make sense)

#calculate the RMSE
model_train_2 <- lm(log_price~log_total_livable_area+number_of_bedrooms
                    +number_of_bathrooms+age+interior_condition+median_income
                    +I(family_hh/total_hh)
                    +pct_bachelors+pct_vacant+pct_white
                    +quality_grade,data = train_data)
test_predictions_2 <- predict(model_train_2, newdata = test_data)
rmse_test_2 <- sqrt(mean((test_data$log_price - test_predictions_2)^2,na.rm = TRUE))
rmse_train_2 <- summary(model_train_2)$sigma



plot(model2)

```
The residual plot for model 2 looks much better than that in model 1. The number of observations are clustered when fitted value is lower decreases, indicating better distribution of residuals. The non-linearity issue also gets better

Q-Q plot still does not look good, especially for the outliers. It seems that outliers is influencing the normality of residuals

There is also heteroskedasticity, as the scale location shows, the line is not linear at all. 

The outlier seems to have small influence on the leverage,as the line is almost linear shown in the last graph

####Model 3: add spatial variables into regression
```{r}
options(scipen = 999)

model3=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
          +interior_condition+median_income+I(family_hh/total_hh)
          +pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
          +mean_3nn_dist+quality_grade,data=for_model_building)

summary(model3)
#percent of family hh become not significant, number of bedrooms become significant again


#check colinearity
vif(model3)
#very sure edu_attainment and mhi may have muticolinearity, make sense EconKDE and its squared term have high vif score as we are taking quadratic term

bptest(model3)
#p-value is way too small, implying more variables may be needed 

#calculate the RMSE
model_train_3 <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
          +interior_condition+median_income+I(family_hh/total_hh)
          +pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
          +mean_3nn_dist+quality_grade,data = train_data)
test_predictions_3 <- predict(model_train_3, newdata = test_data)
rmse_test_3 <- sqrt(mean((test_data$log_price - test_predictions_3)^2,na.rm = TRUE))
rmse_train_3 <- summary(model_train_3)$sigma




plot(model3)

```
The residual plot for model 3 does not change that much compared to model 2. The majority of the issues remain.....

####Model 4: add fixed variable and interaction terms into regression
```{r}
options(scipen = 999)

model4=lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
          +interior_condition+median_income+I(family_hh/total_hh)
          +pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
          +mean_3nn_dist+MHI_quantile*age,data=for_model_building)

summary(model4)
#percent of family hh become not significant, number of bedrooms become significant again


#check colinearity
vif(model4)
#very sure edu_attainment and mhi may have muticolinearity
#tree seems to be correlated with neighborhood (make sense)
#make sense EconKDE and its squared term have high vif score as we are taking quadratic term

bptest(model4)
#p-value is way too small, implying more variables may be needed 

#calculate the RMSE
model_train_4 <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
          +interior_condition+median_income+I(family_hh/total_hh)
          +pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
          +mean_3nn_dist+MHI_quantile*age,data = train_data)
test_predictions_4 <- predict(model_train_4, newdata = test_data)
rmse_test_4 <- sqrt(mean((test_data$log_price - test_predictions_4)^2,na.rm = TRUE))
rmse_train_4 <- summary(model_train_4)$sigma

plot(model4)
```

#one comparison table (RMSE, R² for 4 different models you constructed in your process)
```{r}
rmse_values = c(rmse_test_1,rmse_test_2,rmse_test_3,rmse_test_4)
adj_r2 = c(
  summary(model1)$adj.r.squared,
  summary(model2)$adj.r.squared,
  summary(model3)$adj.r.squared,
  summary(model4)$adj.r.squared
)
model_summary <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
  RMSE = round(rmse_values, 3),
  Adj_R2 = round(adj_r2, 3)
)

stargazer(model_summary,
          type = "html",
          title = "Model Performance Summary",
          summary = FALSE,
          rownames = FALSE,
          out = "model_performance_summary.html")


stargazer(model_summary,
          type = "text",
          title = "Model Performance Summary",
          summary = FALSE,
          rownames = FALSE)
```



#stargazer regression results
```{r}
stargazer(model1,model2,model3,model4,
          type = "html", #could be changed to text if needed
          title = "Model Results",
          dep.var.labels = c("Log of Sale Price"),
          covariate.labels = c(
            "Log(Total Livable Area)",
            "Number of Bedrooms",
            "Number of Bathrooms",
            "Building Age",
            "Interior Condition",
            "Median Household Income",
            "Family Household Share (Family HH / Total HH)",
            "% Bachelor's Degree Holders",
            "% Vacant Housing Units",
            "% White Population",
            "Economic Density (EconKDE)",
            "Economic Density² (EconKDE²)",
            "Tree Density (TreeKDE)",
            "Mean 3-Nearest Neighbor Distance",
            "Income Q2","Income Q3", "Income Q4"
          ),
          omit.stat = c("f", "ser"),
          digits = 3,
          out = "regression.html")

```


#Phase 5: Model Validation

### Set up 10-fold cross-validation
```{r}
set.seed(123)
k <- 10
folds <- cut(seq(1, nrow(for_model_building)), breaks = k, labels = FALSE)
```

### CV for Model 1
```{r}
rmse_cv_1 <- mae_cv_1 <- r2_cv_1 <- numeric(k)

for (i in 1:k) {
  test_indices <- which(folds == i)
  cv_train <- for_model_building[-test_indices, ]
  cv_test <- for_model_building[test_indices, ]
  
  #fit model
  fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
            +interior_condition+quality_grade, data = cv_train)
  preds <- predict(fit, newdata = cv_test)
  
  #calculate metrics
  rmse_cv_1[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
  mae_cv_1[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
  ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
  ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
  r2_cv_1[i] <- 1 - (ss_res / ss_tot)
}

print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_1, 4), 
                 MAE = round(mae_cv_1, 4), R2 = round(r2_cv_1, 4)))
```

### CV for Model 2
```{r}
rmse_cv_2 <- mae_cv_2 <- r2_cv_2 <- numeric(k)

for (i in 1:k) {
  test_indices <- which(folds == i)
  cv_train <- for_model_building[-test_indices, ]
  cv_test <- for_model_building[test_indices, ]
  
  #fit model
  fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
            +interior_condition+median_income+I(family_hh/total_hh)
            +pct_bachelors+pct_vacant+pct_white+quality_grade, data = cv_train)
  preds <- predict(fit, newdata = cv_test)
  
  #calculate metrics
  rmse_cv_2[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
  mae_cv_2[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
  ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
  ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
  r2_cv_2[i] <- 1 - (ss_res / ss_tot)
}

print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_2, 4), 
                 MAE = round(mae_cv_2, 4), R2 = round(r2_cv_2, 4)))
```

### CV for Model 3
```{r}
rmse_cv_3 <- mae_cv_3 <- r2_cv_3 <- numeric(k)

for (i in 1:k) {
  test_indices <- which(folds == i)
  cv_train <- for_model_building[-test_indices, ]
  cv_test <- for_model_building[test_indices, ]
  
  #fit model
  fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
            +interior_condition+median_income+I(family_hh/total_hh)
            +pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
            +mean_3nn_dist+quality_grade, data = cv_train)
  preds <- predict(fit, newdata = cv_test)
  
  #calculate metrics
  rmse_cv_3[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
  mae_cv_3[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
  ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
  ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
  r2_cv_3[i] <- 1 - (ss_res / ss_tot)
}

print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_3, 4), 
                 MAE = round(mae_cv_3, 4), R2 = round(r2_cv_3, 4)))
```

### CV for Model 4
```{r}
rmse_cv_4 <- mae_cv_4 <- r2_cv_4 <- numeric(k)

for (i in 1:k) {
  test_indices <- which(folds == i)
  cv_train <- for_model_building[-test_indices, ]
  cv_test <- for_model_building[test_indices, ]
  
  #fit model
  fit <- lm(log_price~log_total_livable_area+number_of_bedrooms+number_of_bathrooms+age
            +interior_condition+median_income+I(family_hh/total_hh)
            +pct_bachelors+pct_vacant+pct_white+EconKDE+I(EconKDE^2)+TreeKDE
            +mean_3nn_dist+MHI_quantile*age, data = cv_train)
  preds <- predict(fit, newdata = cv_test)
  
  #calculate metrics
  rmse_cv_4[i] <- sqrt(mean((cv_test$log_price - preds)^2, na.rm = TRUE))
  mae_cv_4[i] <- mean(abs(cv_test$log_price - preds), na.rm = TRUE)
  ss_res <- sum((cv_test$log_price - preds)^2, na.rm = TRUE)
  ss_tot <- sum((cv_test$log_price - mean(cv_test$log_price))^2, na.rm = TRUE)
  r2_cv_4[i] <- 1 - (ss_res / ss_tot)
}

print(data.frame(Fold = 1:k, RMSE = round(rmse_cv_4, 4), 
                 MAE = round(mae_cv_4, 4), R2 = round(r2_cv_4, 4)))
```

### CV Results Summary
```{r}
cv_summary <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4"),
  CV_RMSE = round(c(mean(rmse_cv_1), mean(rmse_cv_2), mean(rmse_cv_3), mean(rmse_cv_4)), 3),
  CV_MAE = round(c(mean(mae_cv_1), mean(mae_cv_2), mean(mae_cv_3), mean(mae_cv_4)), 3),
  CV_R2 = round(c(mean(r2_cv_1), mean(r2_cv_2), mean(r2_cv_3), mean(r2_cv_4)), 3)
)

stargazer(cv_summary,
          type = "text",
          title = "10-Fold Cross-Validation Results",
          summary = FALSE,
          rownames = FALSE)

stargazer(cv_summary,
          type = "html",
          title = "10-Fold Cross-Validation Results",
          summary = FALSE,
          rownames = FALSE,
          out = "cv_results.html")
```

### Detailed CV Results by Fold
```{r}
# Show detailed fold-by-fold performance for model 4
detailed_cv <- data.frame(
  Fold = 1:k,
  RMSE = round(rmse_cv_4, 4),
  MAE = round(mae_cv_4, 4),
  R2 = round(r2_cv_4, 4)
)

print(detailed_cv)

# Summary statistics across folds
cat("\nModel 4 CV Performance Summary:\n")
cat("RMSE: Mean =", round(mean(rmse_cv_4), 4), ", SD =", round(sd(rmse_cv_4), 4), "\n")
cat("MAE:  Mean =", round(mean(mae_cv_4), 4), ", SD =", round(sd(mae_cv_4), 4), "\n")
cat("R²:   Mean =", round(mean(r2_cv_4), 4), ", SD =", round(sd(r2_cv_4), 4), "\n")
```

###Predicted vs. Actual Plot
```{r}
#generate predictions using model4
final_predictions <- predict(model4, newdata = for_model_building)

#create scatter plot
plot(for_model_building$log_price, final_predictions,
     xlab = "Actual Log(Sale Price)",
     ylab = "Predicted Log(Sale Price)",
     main = "Predicted vs. Actual Sale Prices",
     pch = 16, col = rgb(0, 0, 1, 0.3))
abline(0, 1, col = "red", lwd = 2, lty = 2)
```
### Feature Importance Analysis
```{r}
coefs <- coef(model4)[-1]

feature_importance <- data.frame(
  Feature = names(coefs),
  Coefficient = round(as.numeric(coefs), 4)
)

print(feature_importance)

#The most important feature is I(EconKDE^2). It has the largest coefficient compared for all the other features. This reflects that being close to commerical activities is really important to housing prices. There is also a pattern that spatial features are important.
```


#Phase 6: Model Diagnostics

## Diagnostic Plots for Best Model
```{r}
#generate diagnostic plots
plot(model4)

#Residual Plot:
#The Residuals vs Fitted plot shows mild heteroskedasticity and a small degree of curvature, which is expected in a dataset of this size, and the model performs consistently well across most of the fitted range with only a few isolated outliers.

#QQ-Plot:
#The Q-Q plot shows slight deviation from normality in the extreme tails, which is common in large real-world housing datasets, while the majority of residuals align closely with the theoretical normal distribution, indicating that model inference remains reliable.

```

## Cook's Distance Plot
```{r}
#identify influential observations
cooks_d <- cooks.distance(model4)
plot(cooks_d, type = "h", main = "Cook's Distance",
     ylab = "Cook's Distance", xlab = "Observation")
abline(h = 4/nrow(for_model_building), col = "red", lty = 2)

#The Cook’s Distance plot shows that while a few observations exert relatively higher influence on the regression model, the vast majority of cases have negligible influence, and no single data point appears to unduly distort the model’s estimated coefficients.
```


#Phase 7: Conclusions & Recommendations

Our final model's is able to explain 64% of the variation on *logged* (is this right way to describe this?) house prices in Philadelphia, with an average error of 48.31%. Our residuals mostly fall along the Q-Q plot's line and are homoscadestic (as seen in the Scale-Location plot), meaning that the residuals both fall on a normal distribution and the variance of residuals is consistent across all independent variable values. Our K-folds analysis returned an R2 of .65 (higher than the model's original .64), illustrating our model's efficacy with non-test data. The feature that matters most is the economic density around the examined housing unit… **[Tim write about econ KDE here]** end of paragraph

Hardest neighborhoods to predict are the most affluent areas — as the social connotations of these spaces are both largely influential in housing prices as well as unquantifiable. Becuase of historical underdevelopment and the history of how that is tied to both race and class, housing prices are intrinsically tied to these vulnerable groups. A model that predicts housing prices is also convincing developers to focus on already well-to-do neighborhoods, whether it be when building housing or commercial centers. There are pros and cons to this, as it may protect neighborhoods from gentrification, but it could also exclude them from capital infusions and access to goods/services. Our model is limited in numerous ways, one of which is the inability to quantify power of word of mouth. For example, once the general vibe around a neighborhood — such as Fishtown — changes from working class (or from another marginalized group) to "up and coming" or "hip", future housing prices in the area may radically change. Another limitation is the non-incorporation of Philadelphia's future plans. For example, if the Roosevelt Boulevard extension was to begin construction, then our model would be unable to predict the change in housing price until the stations were built and the local impacts began.



